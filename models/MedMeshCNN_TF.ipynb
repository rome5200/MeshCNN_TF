{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1035b36-69ce-45f6-a288-4936a930c542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 기본 패키지 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os, trimesh, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 디바이스 설정 (GPU 사용 가능 시 GPU로 설정)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb13ab-2a4f-4991-a241-d29213248174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_npz_cache(mesh_dir, cache_dir):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    mesh_files = [f for f in os.listdir(mesh_dir)\n",
    "                  if f.endswith('.obj') or f.endswith('.stl') or f.endswith('.ply')]\n",
    "\n",
    "    for fname in mesh_files:\n",
    "        mesh_path = os.path.join(mesh_dir, fname)\n",
    "        cache_path = os.path.join(cache_dir, fname.replace('.obj', '.npz').replace('.stl', '.npz').replace('.ply', '.npz'))\n",
    "\n",
    "        # 이미 캐시된 경우 생략\n",
    "        if os.path.exists(cache_path):\n",
    "            print(f\"✅ 존재함: {cache_path} → 건너뜀\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            mesh = trimesh.load(mesh_path, process=False)\n",
    "            vertices = np.array(mesh.vertices)\n",
    "            faces = np.array(mesh.faces)\n",
    "            face_normals = np.array(mesh.face_normals)\n",
    "\n",
    "            np.savez(cache_path, vertices=vertices, faces=faces, face_normals=face_normals)\n",
    "            print(f\"💾 저장됨: {cache_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 실패: {fname} → {e}\")\n",
    "\n",
    "# 🔧 경로 지정\n",
    "source_mesh_dir = r\"C:\\Users\\konyang\\Desktop\\MeshCNN_TF\\data\\dataset\\simplified_mesh\\test\"\n",
    "target_cache_dir = r\"C:\\Users\\konyang\\Desktop\\MeshCNN_TF\\data\\dataset\\cached_mesh\\test\"\n",
    "\n",
    "# 실행\n",
    "generate_npz_cache(source_mesh_dir, target_cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70d996f3-807e-4bcf-9a03-d51f9b850be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LungNoduleDataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train', cache_dir=None):\n",
    "        self.mesh_files = []\n",
    "        self.face_labels = []\n",
    "        self.class_labels = []\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        mesh_dir = os.path.join(data_dir, split)\n",
    "        label_dir = os.path.join(data_dir.replace(\"simplified_mesh\", \"simplified_label\"), split)\n",
    "\n",
    "        if self.cache_dir:\n",
    "            os.makedirs(os.path.join(self.cache_dir, split), exist_ok=True)\n",
    "\n",
    "        for fname in os.listdir(mesh_dir):\n",
    "            if fname.endswith('.obj') or fname.endswith('.stl') or fname.endswith('.ply'):\n",
    "                mesh_path = os.path.join(mesh_dir, fname)\n",
    "                label_path = os.path.join(label_dir, fname.split('.')[0] + \"_label.npy\")\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    face_label = np.load(label_path)\n",
    "                    class_label = 1 if face_label.sum() > 0 else 0\n",
    "                else:\n",
    "                    face_label = None\n",
    "                    class_label = 0\n",
    "\n",
    "                self.mesh_files.append((mesh_path, label_path))\n",
    "                self.class_labels.append(class_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mesh_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mesh_path, label_path = self.mesh_files[idx]\n",
    "        fname = os.path.splitext(os.path.basename(mesh_path))[0]\n",
    "        \n",
    "        try:\n",
    "            # -----------------------------------------\n",
    "            # ✅ 1. 캐시 불러오기 or 생성\n",
    "            # -----------------------------------------\n",
    "            if self.cache_dir:\n",
    "                cached_path = os.path.join(self.cache_dir, fname + \".npz\")\n",
    "                if os.path.exists(cached_path):\n",
    "                    data = np.load(cached_path)\n",
    "                    vertices = data['vertices']\n",
    "                    faces = data['faces']\n",
    "                    face_normals = data['face_normals']\n",
    "                else:\n",
    "                    mesh = trimesh.load(mesh_path)\n",
    "                    vertices = np.array(mesh.vertices)\n",
    "                    faces = np.array(mesh.faces)\n",
    "                    face_normals = np.array(mesh.face_normals)\n",
    "                    np.savez(cached_path, vertices=vertices, faces=faces, face_normals=face_normals)\n",
    "            else:\n",
    "                mesh = trimesh.load(mesh_path)\n",
    "                vertices = np.array(mesh.vertices)\n",
    "                faces = np.array(mesh.faces)\n",
    "                face_normals = np.array(mesh.face_normals)\n",
    "\n",
    "            # -----------------------------------------\n",
    "            # ✅ 2. 레이블 불러오기\n",
    "            # -----------------------------------------\n",
    "            if os.path.exists(label_path):\n",
    "                face_label = np.load(label_path)\n",
    "            else:\n",
    "                face_label = None\n",
    "\n",
    "             # 2. 간선 및 간선->면 매핑 생성\n",
    "            edge_to_faces = {}\n",
    "            for f_idx, face in enumerate(faces):\n",
    "                # 삼각형 face의 세 변 (정렬하여 tuple로 사용)\n",
    "                for e in [(face[0], face[1]), (face[1], face[2]), (face[2], face[0])]:\n",
    "                    e_sorted = tuple(sorted(e))\n",
    "                    if e_sorted not in edge_to_faces:\n",
    "                        edge_to_faces[e_sorted] = []\n",
    "                    edge_to_faces[e_sorted].append(f_idx)\n",
    "            edges = list(edge_to_faces.keys())               # 간선 리스트 (고유 간선)\n",
    "\n",
    "            # 3. 간선 이웃 정보 계산 (각 간선당 최대 4개 이웃 간선)\n",
    "            # 이웃 간선 정의: 하나의 꼭짓점을 공유하는 간선 (1-링 이웃)\n",
    "            vert_to_edges = {v: [] for v in range(len(vertices))}\n",
    "            for e_idx, (v1, v2) in enumerate(edges):\n",
    "                vert_to_edges[v1].append(e_idx)\n",
    "                vert_to_edges[v2].append(e_idx)\n",
    "            neighbors_list = []\n",
    "            for e_idx, (v1, v2) in enumerate(edges):\n",
    "                neighbor_set = set(vert_to_edges[v1] + vert_to_edges[v2])\n",
    "                neighbor_set.discard(e_idx)  # 자기 자신은 제외\n",
    "                neighbors = list(neighbor_set)\n",
    "                # 최대 4개까지 이웃 간선을 선택 (많으면 자르기)\n",
    "                neighbors = neighbors[:4]\n",
    "                # 4개 미만이면 자기 자신으로 패딩하여 크기 고정\n",
    "                while len(neighbors) < 4:\n",
    "                    neighbors.append(e_idx)\n",
    "                neighbors_list.append(neighbors)\n",
    "            neighbor_index = torch.tensor(neighbors_list, dtype=torch.long)\n",
    "\n",
    "            # 4. 간선 특징 계산 (5차원: dihedral, inner1, inner2, ratio1, ratio2)\n",
    "            edge_features = []\n",
    "            for e_idx, (v1, v2) in enumerate(edges):\n",
    "                p1, p2 = vertices[v1], vertices[v2]\n",
    "                # 간선 길이\n",
    "                edge_length = np.linalg.norm(p1 - p2)\n",
    "                # 간선을 공유하는 면 목록\n",
    "                faces_indices = edge_to_faces[(v1, v2)]\n",
    "                # Dihedral angle 계산\n",
    "                if len(faces_indices) == 2:\n",
    "                    f1, f2 = faces_indices[0], faces_indices[1]\n",
    "                    n1, n2 = face_normals[f1], face_normals[f2]\n",
    "                    cos_theta = np.dot(n1, n2) / (np.linalg.norm(n1)*np.linalg.norm(n2) + 1e-8)\n",
    "                    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n",
    "                    dihedral = np.arccos(cos_theta)\n",
    "                else:\n",
    "                    # 인접 면이 하나뿐인 경우 (경계 간선) - dihedral을 0으로 처리\n",
    "                    dihedral = 0.0\n",
    "\n",
    "                # 첫 번째 면의 대향각 및 비율\n",
    "                inner1 = 0.0; ratio1 = 0.0\n",
    "                if len(faces_indices) > 0:\n",
    "                    f1 = faces_indices[0]\n",
    "                    # 간선의 반대쪽 꼭짓점\n",
    "                    face_v = faces[f1]\n",
    "                    # 간선을 이루는 두 꼭짓점을 제외한 나머지 한 꼭짓점\n",
    "                    opp_v = [v for v in face_v if v not in (v1, v2)][0]\n",
    "                    vec1 = vertices[v1] - vertices[opp_v]\n",
    "                    vec2 = vertices[v2] - vertices[opp_v]\n",
    "                    cos_inner = np.dot(vec1, vec2) / ((np.linalg.norm(vec1)*np.linalg.norm(vec2)) + 1e-8)\n",
    "                    cos_inner = np.clip(cos_inner, -1.0, 1.0)\n",
    "                    inner1 = np.arccos(cos_inner)\n",
    "                    # 높이 계산 (삼각형 면적 이용)\n",
    "                    area = np.linalg.norm(np.cross(vec1, vec2)) / 2.0\n",
    "                    height = (2.0 * area) / (edge_length + 1e-8)\n",
    "                    ratio1 = edge_length / (height + 1e-8)\n",
    "                # 두 번째 면의 대향각 및 비율 (없으면 첫 번째 면 값으로 대체)\n",
    "                inner2 = inner1; ratio2 = ratio1\n",
    "                if len(faces_indices) == 2:\n",
    "                    f2 = faces_indices[1]\n",
    "                    face_v2 = faces[f2]\n",
    "                    opp_v2 = [v for v in face_v2 if v not in (v1, v2)][0]\n",
    "                    vec3 = vertices[v1] - vertices[opp_v2]\n",
    "                    vec4 = vertices[v2] - vertices[opp_v2]\n",
    "                    cos_inner2 = np.dot(vec3, vec4) / ((np.linalg.norm(vec3)*np.linalg.norm(vec4)) + 1e-8)\n",
    "                    cos_inner2 = np.clip(cos_inner2, -1.0, 1.0)\n",
    "                    inner2 = np.arccos(cos_inner2)\n",
    "                    area2 = np.linalg.norm(np.cross(vec3, vec4)) / 2.0\n",
    "                    height2 = (2.0 * area2) / (edge_length + 1e-8)\n",
    "                    ratio2 = edge_length / (height2 + 1e-8)\n",
    "                # 특징 벡터 구성\n",
    "                edge_features.append([dihedral, inner1, inner2, ratio1, ratio2])\n",
    "\n",
    "            edge_features = torch.tensor(edge_features, dtype=torch.float32)\n",
    "\n",
    "            # 5. 분할 레이블 (간선 단위) 생성\n",
    "            if face_label is not None:\n",
    "                face_labels_arr = face_label # numpy array of shape (F,)\n",
    "                edge_labels = []\n",
    "                for e_idx, (v1, v2) in enumerate(edges):\n",
    "                    faces_indices = edge_to_faces[(v1, v2)]\n",
    "                    # 해당 간선에 인접한 face 중 하나라도 결절(1)인 경우 간선 레이블 1\n",
    "                    label = 0\n",
    "                    for f_idx in faces_indices:\n",
    "                        if face_labels_arr[f_idx] == 1:\n",
    "                            label = 1\n",
    "                            break\n",
    "                    edge_labels.append(label)\n",
    "                edge_labels = torch.tensor(edge_labels, dtype=torch.long)\n",
    "            else:\n",
    "                # face 레이블이 없는 경우 (분할 라벨이 없으면 모두 0으로 처리)\n",
    "                edge_labels = torch.zeros(len(edges), dtype=torch.long)\n",
    "\n",
    "            # 6. 클래스 레이블 (결절 유무)\n",
    "            class_label = torch.tensor(self.class_labels[idx], dtype=torch.long)\n",
    "\n",
    "            # 결과 반환: 특징, 이웃정보, 클래스 레이블, 간선 레이블, (필요하면 원본 메쉬 정보도 반환 가능)\n",
    "            sample = {\n",
    "                'edge_features': edge_features,          # 계산된 간선 특징 (E, 5)\n",
    "                'neighbor_index': neighbor_index,        # 이웃 간선 인덱스 (E, 4)\n",
    "                'class_label': class_label,              # 이진 클래스 레이블 (0 or 1)\n",
    "                'edge_labels': edge_labels,              # 각 간선에 대한 분할 레이블 (E,)\n",
    "                'vertices': vertices,                    # (V, 3) 시각화용\n",
    "                'faces': faces                           # (F, 3) 시각화용\n",
    "            }\n",
    "            return sample\n",
    "        except Exception as e:\n",
    "            print(f\"❌ __getitem__() error at idx {idx}: {e}\")\n",
    "            \n",
    "            return {\n",
    "                'edge_features': torch.empty(0),\n",
    "                'neighbor_index': torch.empty(0, dtype=torch.long),\n",
    "                'class_label': torch.tensor(0, dtype=torch.long),\n",
    "                'edge_labels': torch.empty(0, dtype=torch.long),\n",
    "                'vertices': np.zeros((0, 3)),\n",
    "                'faces': np.zeros((0, 3))\n",
    "            }\n",
    "\n",
    "# 데이터셋 및 데이터로더 초기화 예시 (파일 경로는 실제 데이터셋에 맞게 수정해야 함)\n",
    "# 경로 설정\n",
    "data_dir = r\"C:\\Users\\konyang\\Desktop\\MeshCNN_TF\\data\\dataset\\simplified_mesh\"\n",
    "cache_dir = r\"C:\\Users\\konyang\\Desktop\\MeshCNN_TF\\data\\dataset\\cached_mesh\"\n",
    "\n",
    "# Dataset 객체 생성 (캐시 경로 포함)\n",
    "train_dataset = LungNoduleDataset(\n",
    "    data_dir=data_dir,\n",
    "    split='train',\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# DataLoader 설정 (병렬 처리 권장)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # ✅ 워커 병렬 처리 끄기\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ✅ 검증용 데이터셋 생성\n",
    "val_dataset = LungNoduleDataset(\n",
    "    data_dir=data_dir,\n",
    "    split='val',\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,  # 검증은 shuffle하지 않음\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d34f4225-68ec-4b26-ad93-6a6a6911b5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeshConv(nn.Module):\n",
    "    \"\"\"메쉬 간선 합성곱 레이어: 각 간선과 이웃한 4개 간선 (총5개)의 특징을 통합해 출력.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MeshConv, self).__init__()\n",
    "        # 5 * in_channels 크기의 입력을 out_channels로 변환\n",
    "        self.linear = nn.Linear(in_channels * 5, out_channels)\n",
    "    \n",
    "    def forward(self, x, neighbor_index):\n",
    "        # x: (E, in_channels)\n",
    "        # neighbor_index: (E, 4)\n",
    "\n",
    "        E, in_channels = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # (E, 1, in_channels): 자기 자신\n",
    "        x_self = x.unsqueeze(1)\n",
    "\n",
    "        # (E, 4) → (E, 4, in_channels): neighbor 인덱스를 gather하기 위해 확장\n",
    "        neighbor_index_expanded = neighbor_index.unsqueeze(-1).expand(-1, -1, in_channels)\n",
    "\n",
    "        # x → (1, E, in_channels): gather dim=1을 위해 확장\n",
    "        x_expanded = x.unsqueeze(0)\n",
    "\n",
    "        # gather를 사용해 neighbor feature 추출: (1, E, C)에서 gather (E, 4, C)\n",
    "        x_neighbors = torch.gather(x_expanded.expand(E, -1, -1), dim=1, index=neighbor_index_expanded)\n",
    "\n",
    "        # concat: (E, 5, in_channels)\n",
    "        x_combined = torch.cat([x_self, x_neighbors], dim=1)\n",
    "\n",
    "        # Flatten: (E, 5 * in_channels)\n",
    "        x_flat = x_combined.view(E, -1)\n",
    "\n",
    "        out = self.linear(x_flat)\n",
    "        return out  # (E, out_channels)\n",
    "\n",
    "class MeshPool(nn.Module):\n",
    "    \"\"\"메쉬 풀링 레이어: 간선 수를 감소 (2:1 비율로 클러스터).\"\"\"\n",
    "    def __init__(self):\n",
    "        super(MeshPool, self).__init__()\n",
    "        self.cluster_map = None  # 나중에 unpool에 사용하기 위한 매핑\n",
    "    \n",
    "    def forward(self, x, neighbor_index):\n",
    "        # x: (N, C) 입력 간선 특징, N = 현재 간선 수\n",
    "        N = x.size(0)\n",
    "        # 클러스터 구성: 2개 간선씩 묶음 (홀수개일 경우 마지막은 단독 클러스터)\n",
    "        if N % 2 == 0:\n",
    "            new_N = N // 2\n",
    "        else:\n",
    "            new_N = N // 2 + 1\n",
    "        # 출력 텐서 초기화\n",
    "        device = x.device\n",
    "        C = x.size(1)\n",
    "        x_pooled = torch.zeros((new_N, C), dtype=x.dtype, device=device)\n",
    "        # cluster_map: 길이 N 리스트로, 각 간선이 속한 클러스터 인덱스\n",
    "        cluster_map = [-1] * N\n",
    "        # 2개씩 평균\n",
    "        for i in range(N // 2):\n",
    "            idx1 = 2 * i\n",
    "            idx2 = 2 * i + 1\n",
    "            x_pooled[i] = 0.5 * (x[idx1] + x[idx2])\n",
    "            cluster_map[idx1] = i\n",
    "            cluster_map[idx2] = i\n",
    "        if N % 2 == 1:\n",
    "            # 마지막 간선은 단독 클러스터\n",
    "            x_pooled[new_N - 1] = x[N - 1]\n",
    "            cluster_map[N - 1] = new_N - 1\n",
    "        # cluster_map을 Tensor로 저장\n",
    "        cluster_map = torch.tensor(cluster_map, dtype=torch.long, device=device)\n",
    "        self.cluster_map = cluster_map\n",
    "        \n",
    "        # 새로운 neighbor_index 계산 (클러스터 그래프의 이웃)\n",
    "        neighbors_coarse = []\n",
    "        # 원래 간선의 neighbor_index로부터 클러스터 간 이웃을 유추\n",
    "        for edge_idx in range(N):\n",
    "            cluster_idx = cluster_map[edge_idx].item()\n",
    "            # 원래 간선의 이웃들의 클러스터 인덱스\n",
    "            for nbr_edge in neighbor_index[edge_idx]:\n",
    "                nbr_cluster = cluster_map[nbr_edge].item()\n",
    "                if nbr_cluster != cluster_idx:\n",
    "                    # 리스트 길이를 cluster_idx에 맞춰 확장\n",
    "                    if cluster_idx >= len(neighbors_coarse):\n",
    "                        neighbors_coarse.extend([set() for _ in range(cluster_idx - len(neighbors_coarse) + 1)])\n",
    "                    neighbors_coarse[cluster_idx].add(nbr_cluster)\n",
    "        # 집합을 리스트로 변환하고 4개로 패딩\n",
    "        for i in range(len(neighbors_coarse)):\n",
    "            nbrs = list(neighbors_coarse[i])\n",
    "            nbrs = nbrs[:4]  # 최대 4개까지 사용\n",
    "            while len(nbrs) < 4:\n",
    "                nbrs.append(i)  # 자기 자신으로 패딩\n",
    "            neighbors_coarse[i] = nbrs\n",
    "        # 만약 어떤 클러스터에 대해 neighbor_set이 비어있으면 자기 자신 4개로\n",
    "        if len(neighbors_coarse) < new_N:\n",
    "            # 빈 클러스터 neighbor 세트 초기화\n",
    "            for i in range(len(neighbors_coarse), new_N):\n",
    "                neighbors_coarse.append([i, i, i, i])\n",
    "        neighbor_index_coarse = torch.tensor(neighbors_coarse, dtype=torch.long, device=device)\n",
    "        \n",
    "        return x_pooled, neighbor_index_coarse\n",
    "\n",
    "class MeshUnpool(nn.Module):\n",
    "    \"\"\"메쉬 업풀링 레이어: 풀링된 간선 특징을 원래 개수로 복원.\"\"\"\n",
    "    def forward(self, x_coarse, cluster_map):\n",
    "        # x_coarse: (M, C) 풀링된 간선 특징, cluster_map: (N,) 각 원래 간선 -> 클러스터 인덱스 매핑\n",
    "        device = x_coarse.device\n",
    "        N = cluster_map.shape[0]  # 원래 간선 개수\n",
    "        C = x_coarse.shape[1]\n",
    "        # cluster_map을 이용해 각 원래 간선의 특징 할당\n",
    "        x_reconstructed = torch.zeros((N, C), dtype=x_coarse.dtype, device=device)\n",
    "        # cluster_map의 각 인덱스를 순회하며 할당\n",
    "        for orig_edge_idx, cluster_idx in enumerate(cluster_map):\n",
    "            x_reconstructed[orig_edge_idx] = x_coarse[cluster_idx]\n",
    "        return x_reconstructed\n",
    "\n",
    "# 이제 Encoder, Decoder, Classifier, Segmenter를 정의\n",
    "class MeshEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=5):\n",
    "        super(MeshEncoder, self).__init__()\n",
    "        # 채널 설정: conv1_out=16, conv2_out=32, conv3_out=64 (예시)\n",
    "        self.conv1 = MeshConv(in_channels, 16)\n",
    "        self.pool1 = MeshPool()\n",
    "        self.conv2 = MeshConv(16, 32)\n",
    "        self.pool2 = MeshPool()\n",
    "        self.conv3 = MeshConv(32, 64)\n",
    "    \n",
    "    def forward(self, x, neighbor_index):\n",
    "        # conv1 + ReLU\n",
    "        x1 = F.relu(self.conv1(x, neighbor_index))\n",
    "        # pool1\n",
    "        x_pooled1, neighbor_coarse1 = self.pool1(x1, neighbor_index)\n",
    "        # conv2 + ReLU on pooled1\n",
    "        x2 = F.relu(self.conv2(x_pooled1, neighbor_coarse1))\n",
    "        # pool2\n",
    "        x_pooled2, neighbor_coarse2 = self.pool2(x2, neighbor_coarse1)\n",
    "        # conv3 + ReLU on pooled2 (encoder 최종)\n",
    "        x3 = F.relu(self.conv3(x_pooled2, neighbor_coarse2))\n",
    "        # Encoder 결과와 중간 결과 반환 (스킵 연결 및 Decoder에 필요)\n",
    "        return {\n",
    "            'x1': x1,                       # original level features (E edges, 16ch)\n",
    "            'neighbor0': neighbor_index, \n",
    "            'x2': x2,                       # half level features (E/2 edges, 32ch)\n",
    "            'neighbor1': neighbor_coarse1,\n",
    "            'x3': x3,                       # quarter level features (E/4 edges, 64ch)\n",
    "            'neighbor2': neighbor_coarse2,\n",
    "            'pool1': self.pool1,\n",
    "            'pool2': self.pool2\n",
    "        }\n",
    "\n",
    "class MeshDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeshDecoder, self).__init__()\n",
    "        self.conv2_up = MeshConv(32 + 64, 32)\n",
    "        self.conv1_up = MeshConv(16 + 32, 16)\n",
    "\n",
    "    def forward(self, enc_out):\n",
    "        x2 = enc_out['x2']\n",
    "        x3 = enc_out['x3']\n",
    "        neighbor1 = enc_out['neighbor1']\n",
    "        neighbor0 = enc_out['neighbor0']\n",
    "        pool1_layer = enc_out['pool1']\n",
    "        pool2_layer = enc_out['pool2']\n",
    "\n",
    "        # Unpool quarter → half\n",
    "        x2_up = MeshUnpool().forward(x3.detach(), pool2_layer.cluster_map)  # (E/2, 64)\n",
    "\n",
    "        # Concat with skip connection\n",
    "        x2_cat = torch.cat([x2.detach(), x2_up], dim=1)  # (E/2, 96)\n",
    "        x2_up_refined = F.relu(self.conv2_up(x2_cat, neighbor1))  # (E/2, 32)\n",
    "\n",
    "        # Unpool half → original\n",
    "        x1_up = MeshUnpool().forward(x2_up_refined.detach(), pool1_layer.cluster_map)  # (E, 32)\n",
    "\n",
    "        x1 = enc_out['x1']\n",
    "        x1_cat = torch.cat([x1.detach(), x1_up], dim=1)  # (E, 48)\n",
    "\n",
    "        x1_up_refined = F.relu(self.conv1_up(x1_cat, neighbor0))  # (E, 16)\n",
    "\n",
    "        return x1_up_refined\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim=100, num_classes=2):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        # 단순한 MLP 분류기: [in_channels] -> [hidden_dim] -> [num_classes]\n",
    "        self.fc1 = nn.Linear(in_channels, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (in_channels) or (1, in_channels) 전역 특징 벡터\n",
    "        x = F.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out  # raw logits (num_classes)\n",
    "    \n",
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=2):\n",
    "        super(SegmentationHead, self).__init__()\n",
    "        self.linear = nn.Linear(in_channels, num_classes)\n",
    "    def forward(self, x):\n",
    "        # x: (E, in_channels) 모든 원래 간선에 대한 복원된 특징\n",
    "        out = self.linear(x)  # (E, num_classes)\n",
    "        return out\n",
    "\n",
    "# 전체 모델 통합\n",
    "class MedMeshNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MedMeshNet, self).__init__()\n",
    "        self.encoder = MeshEncoder(in_channels=5)\n",
    "        self.decoder = MeshDecoder()\n",
    "        # classification은 encoder 최종 출력 채널 (예: 64)을 받아 이진 분류\n",
    "        self.classifier = ClassificationHead(in_channels=64, hidden_dim=100, num_classes=2)\n",
    "        # segmentation은 decoder 최종 출력 채널 (예: 16)을 받아 2-class 출력\n",
    "        self.segmenter = SegmentationHead(in_channels=16, num_classes=2)\n",
    "    \n",
    "    def forward(self, edge_features, neighbor_index):\n",
    "        # 1. Encoder: 특징 추출 및 다운샘플\n",
    "        enc_out = self.encoder(edge_features, neighbor_index)\n",
    "        # 2. Classification: Encoder 최종 특징들을 전역 요약하여 클래스 예측\n",
    "        x_global = torch.max(enc_out['x3'], dim=0)[0]  # 전역 max 풀 (64차원 벡터)\n",
    "        class_logits = self.classifier(x_global.unsqueeze(0))  # (1,2) 출력\n",
    "        # 3. Decoder: 업샘플로 원래 해상도 특징 복원\n",
    "        dec_out = self.decoder(enc_out)  # (E, 16)\n",
    "        seg_logits = self.segmenter(dec_out)  # (E, 2)\n",
    "        return class_logits, seg_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f29ce-e5f9-45d0-a1e7-72073258fd30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== [Epoch 1] ==========\n",
      "244/244 [100%] - 10.2s/step - loss: 0.7902 - acc: 0.6808\n",
      "✅ Epoch 1 완료 - 평균 손실: 0.7902, 정확도: 68.08%, 시간: 3024.9s\n",
      "70/70 [100%] - 8.3s/step - val_loss: 0.6880 - val_acc: 0.7286\n",
      "🧪 검증 완료 - 평균 손실: 0.6880, 정확도: 72.86%, IoU: 0.9941, 시간: 790.5s\n",
      "\n",
      "========== [Epoch 2] ==========\n",
      "244/244 [100%] - 10.0s/step - loss: 0.6100 - acc: 0.7183\n",
      "✅ Epoch 2 완료 - 평균 손실: 0.6100, 정확도: 71.83%, 시간: 3007.7s\n",
      "70/70 [100%] - 8.3s/step - val_loss: 0.6813 - val_acc: 0.7286\n",
      "🧪 검증 완료 - 평균 손실: 0.6813, 정확도: 72.86%, IoU: 0.9941, 시간: 791.4s\n",
      "\n",
      "========== [Epoch 3] ==========\n",
      "244/244 [100%] - 10.4s/step - loss: 0.5783 - acc: 0.7371\n",
      "✅ Epoch 3 완료 - 평균 손실: 0.5783, 정확도: 73.71%, 시간: 3014.9s\n",
      "70/70 [100%] - 8.3s/step - val_loss: 0.6613 - val_acc: 0.7286\n",
      "🧪 검증 완료 - 평균 손실: 0.6613, 정확도: 72.86%, IoU: 0.9941, 시간: 791.5s\n",
      "\n",
      "========== [Epoch 4] ==========\n",
      "243/244 [100%] - 10.4s/step - loss: 0.5837 - acc: 0.7465\n",
      "✅ Epoch 4 완료 - 평균 손실: 0.5837, 정확도: 74.65%, 시간: 3028.1s\n",
      "70/70 [100%] - 8.3s/step - val_loss: 0.7452 - val_acc: 0.7143\n",
      "🧪 검증 완료 - 평균 손실: 0.7452, 정확도: 71.43%, IoU: 0.9941, 시간: 791.9s\n",
      "\n",
      "========== [Epoch 5] ==========\n",
      "244/244 [100%] - 11.7s/step - loss: 0.5943 - acc: 0.7418\n",
      "✅ Epoch 5 완료 - 평균 손실: 0.5943, 정확도: 74.18%, 시간: 3008.4s\n",
      "70/70 [100%] - 8.5s/step - val_loss: 0.6763 - val_acc: 0.7286\n",
      "🧪 검증 완료 - 평균 손실: 0.6763, 정확도: 72.86%, IoU: 0.9941, 시간: 793.7s\n",
      "\n",
      "========== [Epoch 6] ==========\n",
      "103/244 [42%] - 10.0s/step - loss: 0.4643 - acc: 0.8046\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 모델 초기화\n",
    "model = MedMeshNet().to(device)\n",
    "\n",
    "# 손실 함수 정의\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "class_weights = torch.tensor([1.0, 5.0], device=device)  # ✅ 결절 가중치 강화\n",
    "criterion_seg = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# 옵티마이저 & 스케줄러\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# AMP용 스케일러\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# 검증용 val_loader 존재 확인\n",
    "try:\n",
    "    val_loader\n",
    "except NameError:\n",
    "    val_loader = None\n",
    "\n",
    "# ---------------------------\n",
    "# AMP 적용 학습 루프 시작\n",
    "# ---------------------------\n",
    "num_epochs = 10\n",
    "total_steps = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    print(f\"\\n========== [Epoch {epoch}] ==========\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        step_start = time.time()\n",
    "\n",
    "        features = batch['edge_features'].to(device).squeeze(0)\n",
    "        if features.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        neighbor_index = batch['neighbor_index'].to(device).squeeze(0)\n",
    "        class_label = batch['class_label'].to(device)\n",
    "        seg_label = batch['edge_labels'].to(device).squeeze(0)\n",
    "\n",
    "        if features.shape[0] > 25000:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            class_logits, seg_logits = model(features, neighbor_index)\n",
    "            loss_class = criterion_class(class_logits, class_label)\n",
    "            loss_seg = criterion_seg(seg_logits, seg_label)\n",
    "            loss = loss_class + loss_seg\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred_class = torch.argmax(class_logits, dim=1)\n",
    "        correct_train += (pred_class == class_label).sum().item()\n",
    "        total_train += class_label.size(0)\n",
    "\n",
    "        # Keras-style 출력\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "        train_acc = correct_train / total_train if total_train > 0 else 0\n",
    "        elapsed = time.time() - step_start\n",
    "        print(f\"{i+1}/{total_steps} [{(i+1)/total_steps:.0%}] - {elapsed:.1f}s/step - loss: {avg_loss:.4f} - acc: {train_acc:.4f}\", end=\"\\r\")\n",
    "\n",
    "    total_elapsed = time.time() - start_time\n",
    "    print(f\"\\n✅ Epoch {epoch} 완료 - 평균 손실: {avg_loss:.4f}, 정확도: {train_acc*100:.2f}%, 시간: {total_elapsed:.1f}s\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # ---------------------------\n",
    "    # 검증 단계 (no_grad만 사용)\n",
    "    # ---------------------------\n",
    "    if val_loader is not None:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        inter = 0\n",
    "        union = 0\n",
    "        val_loss = 0.0\n",
    "        val_start = time.time()\n",
    "\n",
    "        for j, batch in enumerate(val_loader):\n",
    "            step_start = time.time()\n",
    "\n",
    "            features = batch['edge_features'].to(device).squeeze(0)\n",
    "            if features.numel() == 0:\n",
    "                continue\n",
    "            neighbor_index = batch['neighbor_index'].to(device).squeeze(0)\n",
    "            class_label = batch['class_label'].to(device)\n",
    "            seg_label = batch['edge_labels'].to(device).squeeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                class_logits, seg_logits = model(features, neighbor_index)\n",
    "                loss_class = criterion_class(class_logits, class_label)\n",
    "                loss_seg = criterion_seg(seg_logits, seg_label)\n",
    "                loss = loss_class + loss_seg\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                pred_class = torch.argmax(class_logits, dim=1)\n",
    "                correct += (pred_class == class_label).sum().item()\n",
    "                total_samples += class_label.size(0)\n",
    "\n",
    "                pred_seg = torch.argmax(seg_logits, dim=1)\n",
    "                num_classes = seg_logits.shape[1]\n",
    "                for cls in range(num_classes):\n",
    "                    inter += ((pred_seg == cls) & (seg_label == cls)).sum().item()\n",
    "                    union += ((pred_seg == cls) | (seg_label == cls)).sum().item()\n",
    "\n",
    "            # 실시간 출력\n",
    "            avg_vloss = val_loss / (j + 1)\n",
    "            val_acc = correct / total_samples if total_samples > 0 else 0\n",
    "            elapsed = time.time() - step_start\n",
    "            print(f\"{j+1}/{val_steps} [{(j+1)/val_steps:.0%}] - {elapsed:.1f}s/step - val_loss: {avg_vloss:.4f} - val_acc: {val_acc:.4f}\", end='\\r')\n",
    "\n",
    "        val_elapsed = time.time() - val_start\n",
    "        val_acc = correct / total_samples if total_samples > 0 else 0\n",
    "        val_iou = inter / (union + 1e-8) if union > 0 else 0\n",
    "        print(f\"\\n🧪 검증 완료 - 평균 손실: {avg_vloss:.4f}, 정확도: {val_acc * 100:.2f}%, IoU: {val_iou:.4f}, 시간: {val_elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193d949-6b61-454f-8bb8-eaf9e7214ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            features = batch['edge_features'].to(device)\n",
    "            neighbor_index = batch['neighbor_index'].to(device)\n",
    "            labels = batch['class_label'].to(device)\n",
    "            class_logits, _ = model(features, neighbor_index)\n",
    "            pred = torch.argmax(class_logits, dim=1)  # 예측 클래스\n",
    "            if pred.item() == labels.item():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    return acc\n",
    "\n",
    "def compute_segmentation_iou(model, data_loader):\n",
    "    model.eval()\n",
    "    inter = 0\n",
    "    union = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            features = batch['edge_features'].to(device)\n",
    "            neighbor_index = batch['neighbor_index'].to(device)\n",
    "            true_seg = batch['edge_labels'].to(device)\n",
    "            _, seg_logits = model(features, neighbor_index)\n",
    "            pred_seg = torch.argmax(seg_logits, dim=1)\n",
    "            # 결절 클래스(1)에 대한 IoU 계산\n",
    "            inter += torch.logical_and(pred_seg == 1, true_seg == 1).sum().item()\n",
    "            union += torch.logical_or(pred_seg == 1, true_seg == 1).sum().item()\n",
    "    iou = inter / (union + 1e-8) if union > 0 else 0\n",
    "    return iou\n",
    "\n",
    "# 예시: 학습 완료 후 테스트 세트에 대한 성능 측정\n",
    "\n",
    "test_acc = compute_classification_accuracy(model, test_loader)\n",
    "test_iou = compute_segmentation_iou(model, test_loader)\n",
    "print(f\"테스트 정확도: {test_acc:.4f}, 테스트 IoU: {test_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f305c-f902-4695-ace0-43183cb78971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ✅ 모델 준비\n",
    "model.eval()\n",
    "\n",
    "# ✅ 파일 이름 지정\n",
    "target_filename = \"A0173_abnormal.obj\"\n",
    "\n",
    "# ✅ 경로 설정\n",
    "original_mesh_dir = r\"C:\\Users\\konyang\\Desktop\\MeshCNN_TF\\data\\dataset\\mesh\"\n",
    "cache_dir = r\"C:\\Users\\konyang\\Desktop\\MeshCNN_TF\\data\\dataset\\cached_mesh\\train\"\n",
    "npz_path = os.path.join(cache_dir, target_filename.replace(\".obj\", \".npz\"))\n",
    "\n",
    "# ✅ 캐시된 특징 불러오기\n",
    "if not os.path.exists(npz_path):\n",
    "    raise FileNotFoundError(f\"{npz_path} 캐시 파일이 없습니다.\")\n",
    "\n",
    "data = np.load(npz_path)\n",
    "vertices = data['vertices']\n",
    "faces = data['faces']\n",
    "face_normals = data['face_normals']\n",
    "\n",
    "# ✅ 간선 및 이웃 인덱스 계산\n",
    "edge_to_faces = {}\n",
    "for f_idx, face in enumerate(faces):\n",
    "    for e in [(face[0], face[1]), (face[1], face[2]), (face[2], face[0])]:\n",
    "        e_sorted = tuple(sorted(e))\n",
    "        edge_to_faces.setdefault(e_sorted, []).append(f_idx)\n",
    "edges = list(edge_to_faces.keys())\n",
    "\n",
    "# 이웃 인덱스 계산\n",
    "vert_to_edges = {v: [] for v in range(len(vertices))}\n",
    "for e_idx, (v1, v2) in enumerate(edges):\n",
    "    vert_to_edges[v1].append(e_idx)\n",
    "    vert_to_edges[v2].append(e_idx)\n",
    "\n",
    "neighbors_list = []\n",
    "for e_idx, (v1, v2) in enumerate(edges):\n",
    "    neighbor_set = set(vert_to_edges[v1] + vert_to_edges[v2])\n",
    "    neighbor_set.discard(e_idx)\n",
    "    neighbors = list(neighbor_set)[:4]\n",
    "    while len(neighbors) < 4:\n",
    "        neighbors.append(e_idx)\n",
    "    neighbors_list.append(neighbors)\n",
    "\n",
    "neighbor_index = torch.tensor(neighbors_list, dtype=torch.long)\n",
    "\n",
    "# 간선 특징 계산\n",
    "edge_features = []\n",
    "for e_idx, (v1, v2) in enumerate(edges):\n",
    "    p1, p2 = vertices[v1], vertices[v2]\n",
    "    edge_length = np.linalg.norm(p1 - p2)\n",
    "    faces_indices = edge_to_faces[(v1, v2)]\n",
    "    if len(faces_indices) == 2:\n",
    "        f1, f2 = faces_indices[0], faces_indices[1]\n",
    "        n1, n2 = face_normals[f1], face_normals[f2]\n",
    "        cos_theta = np.dot(n1, n2) / (np.linalg.norm(n1)*np.linalg.norm(n2) + 1e-8)\n",
    "        dihedral = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
    "    else:\n",
    "        dihedral = 0.0\n",
    "\n",
    "    def compute_inner_ratio(f_idx):\n",
    "        face_v = faces[f_idx]\n",
    "        opp_v = [v for v in face_v if v not in (v1, v2)][0]\n",
    "        vec1 = vertices[v1] - vertices[opp_v]\n",
    "        vec2 = vertices[v2] - vertices[opp_v]\n",
    "        cos_inner = np.dot(vec1, vec2) / (np.linalg.norm(vec1)*np.linalg.norm(vec2) + 1e-8)\n",
    "        inner = np.arccos(np.clip(cos_inner, -1.0, 1.0))\n",
    "        area = np.linalg.norm(np.cross(vec1, vec2)) / 2.0\n",
    "        height = (2.0 * area) / (edge_length + 1e-8)\n",
    "        ratio = edge_length / (height + 1e-8)\n",
    "        return inner, ratio\n",
    "\n",
    "    inner1, ratio1 = compute_inner_ratio(faces_indices[0]) if faces_indices else (0.0, 0.0)\n",
    "    inner2, ratio2 = compute_inner_ratio(faces_indices[1]) if len(faces_indices) == 2 else (inner1, ratio1)\n",
    "\n",
    "    edge_features.append([dihedral, inner1, inner2, ratio1, ratio2])\n",
    "\n",
    "edge_features = torch.tensor(edge_features, dtype=torch.float32)\n",
    "\n",
    "# ✅ 모델 예측\n",
    "edge_features = edge_features.to(device)\n",
    "neighbor_index = neighbor_index.to(device)\n",
    "with torch.no_grad():\n",
    "    class_logits, seg_logits = model(edge_features, neighbor_index)\n",
    "    pred_class = torch.argmax(class_logits, dim=1).item()\n",
    "    pred_seg = torch.argmax(seg_logits, dim=1).cpu().numpy()\n",
    "\n",
    "print(\"모델 예측:\", \"폐 결절 있음\" if pred_class == 1 else \"결절 없음\")\n",
    "\n",
    "# ✅ 결절로 예측된 face 추출\n",
    "pred_nodule_faces = set()\n",
    "for e_idx, edge in enumerate(edges):\n",
    "    if pred_seg[e_idx] == 1:\n",
    "        for f_idx in edge_to_faces[edge]:\n",
    "            pred_nodule_faces.add(f_idx)\n",
    "\n",
    "pred_nodule_faces = list(pred_nodule_faces)\n",
    "print(f\"예측된 결절 영역 면 개수: {len(pred_nodule_faces)}개\")\n",
    "\n",
    "# ✅ 원본 메쉬 로드 (고해상도 obj)\n",
    "obj_path = os.path.join(original_mesh_dir, target_filename)\n",
    "tm = trimesh.load(obj_path)\n",
    "vertices = np.array(tm.vertices)\n",
    "faces = np.array(tm.faces)\n",
    "\n",
    "# 하이라이트할 정점\n",
    "highlighted_vertices = set()\n",
    "for f_idx in pred_nodule_faces:\n",
    "    if f_idx < len(faces):\n",
    "        highlighted_vertices.update(faces[f_idx])\n",
    "\n",
    "# 색상 할당\n",
    "colors = np.tile([0.7, 0.7, 0.7], (len(vertices), 1))  # 회색\n",
    "for v in highlighted_vertices:\n",
    "    if v < len(colors):\n",
    "        colors[v] = [1.0, 0.0, 0.0]  # 빨강\n",
    "\n",
    "# Open3D 메쉬 변환\n",
    "mesh_o3d = o3d.geometry.TriangleMesh()\n",
    "mesh_o3d.vertices = o3d.utility.Vector3dVector(vertices)\n",
    "mesh_o3d.triangles = o3d.utility.Vector3iVector(faces)\n",
    "mesh_o3d.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "mesh_o3d.compute_vertex_normals()\n",
    "\n",
    "# ✅ 시각화\n",
    "o3d.visualization.draw_geometries([mesh_o3d], window_name=\"결절 예측 시각화\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d1710-b128-46d1-9e51-cf9d9bb14d73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, jaccard_score, accuracy_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds_cls = []\n",
    "    all_labels_cls = []\n",
    "\n",
    "    all_preds_seg = []\n",
    "    all_labels_seg = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features = batch['edge_features'].to(device).squeeze(0)\n",
    "            neighbor_index = batch['neighbor_index'].to(device).squeeze(0)\n",
    "            class_label = batch['class_label'].to(device)\n",
    "            seg_label = batch['edge_labels'].to(device).squeeze(0)\n",
    "\n",
    "            if features.numel() == 0 or features.shape[0] > 25000:\n",
    "                continue\n",
    "\n",
    "            class_logits, seg_logits = model(features, neighbor_index)\n",
    "\n",
    "            # ✅ 분류 결과\n",
    "            pred_class = torch.argmax(class_logits, dim=1)\n",
    "            all_preds_cls.extend(pred_class.cpu().numpy())\n",
    "            all_labels_cls.extend(class_label.cpu().numpy())\n",
    "\n",
    "            # ✅ 세분화 결과\n",
    "            pred_seg = torch.argmax(seg_logits, dim=1)\n",
    "            all_preds_seg.extend(pred_seg.cpu().numpy())\n",
    "            all_labels_seg.extend(seg_label.cpu().numpy())\n",
    "\n",
    "    # ▶ Classification 성능\n",
    "    print(\"🔎 [Classification Results]\")\n",
    "    print(classification_report(all_labels_cls, all_preds_cls, target_names=[\"Normal\", \"Nodule\"]))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels_cls, all_preds_cls))\n",
    "\n",
    "    # ▶ Segmentation 성능 (IoU, 정확도)\n",
    "    print(\"\\n🔎 [Segmentation Results]\")\n",
    "    acc_seg = accuracy_score(all_labels_seg, all_preds_seg)\n",
    "    iou_seg = jaccard_score(all_labels_seg, all_preds_seg, average='binary')\n",
    "    print(f\"Segmentation Accuracy: {acc_seg * 100:.2f}%\")\n",
    "    print(f\"Segmentation IoU: {iou_seg:.4f}\")\n",
    "\n",
    "evaluate_model(model, val_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b85ce8-df33-4b59-bebb-25f16c2d043f",
   "metadata": {},
   "source": [
    "하이퍼파라미터 조정, 데이터 증강, 평가 지표 추가 및 최적화 등 추가 작업 필요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2025",
   "language": "python",
   "name": "project2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
