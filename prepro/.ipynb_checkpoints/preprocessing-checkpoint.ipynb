{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1c5d18-2e37-44ac-a5e9-4b0d49613d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05effe5f-21bb-440b-b530-dec4907b47e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 메쉬 파일을 불러오는 역할\n",
    "# =============================================================================\n",
    "\n",
    "class MeshLoader:\n",
    "    def __init__(self, data_dir: str, supported_formats=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.supported_formats = supported_formats or ['.npy', '.obj', '.ply', '.stl', '.off']\n",
    "        self.mesh_files = self._collect_mesh_files()\n",
    "\n",
    "    def _collect_mesh_files(self) -> List[Path]:\n",
    "        files = []\n",
    "        for ext in self.supported_formats:\n",
    "            files.extend(self.data_dir.glob(f\"**/*{ext}\"))\n",
    "        return sorted(files)\n",
    "\n",
    "    def get_all_file_paths(self) -> List[str]:\n",
    "        return [str(f) for f in self.mesh_files]\n",
    "\n",
    "    def load_mesh_data(self, file_path: str) -> Dict:\n",
    "        try:\n",
    "            mesh = trimesh.load(file_path, force='mesh')\n",
    "            if hasattr(mesh, 'geometry'):\n",
    "                mesh = list(mesh.geometry.values())[0]\n",
    "            vertices = np.array(mesh.vertices, dtype=np.float32)\n",
    "            faces = np.array(mesh.faces, dtype=np.int32)\n",
    "            if len(vertices) == 0 or len(faces) == 0:\n",
    "                raise ValueError(\"빈 메쉬\")\n",
    "            return {'vertices': vertices, 'faces': faces, 'file_path': file_path}\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 로딩 실패 {file_path}: {e}\")\n",
    "            return self._create_dummy_mesh()\n",
    "\n",
    "    def _create_dummy_mesh(self) -> Dict:\n",
    "        vertices = np.array([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [1.0, 0.0, 0.0],\n",
    "            [0.5, 0.866, 0.0],\n",
    "            [0.5, 0.289, 0.816]\n",
    "        ], dtype=np.float32)\n",
    "        faces = np.array([\n",
    "            [0, 1, 2],\n",
    "            [0, 2, 3],\n",
    "            [0, 3, 1],\n",
    "            [1, 3, 2]\n",
    "        ], dtype=np.int32)\n",
    "        return {'vertices': vertices, 'faces': faces, 'file_path': 'dummy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c70f04-3483-4317-92d4-1bc36b8c1f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeshPreprocessor:\n",
    "    def __init__(self, max_vertices: int, max_faces: int):\n",
    "        self.max_vertices = max_vertices\n",
    "        self.max_faces = max_faces\n",
    "\n",
    "    def preprocess(self, mesh_data: Dict) -> Dict:\n",
    "        vertices = self._normalize_vertices(mesh_data['vertices'])\n",
    "        vertices, faces = self._resize_mesh(vertices, mesh_data['faces'])\n",
    "        normals = self._compute_face_normals(vertices, faces)\n",
    "        features = self._compute_edge_features(vertices, faces)\n",
    "\n",
    "        return {\n",
    "            'vertices': tf.constant(vertices, dtype=tf.float32),\n",
    "            'faces': tf.constant(faces, dtype=tf.int32),\n",
    "            'normals': tf.constant(normals, dtype=tf.float32),\n",
    "            'features': tf.constant(features, dtype=tf.float32)\n",
    "        }\n",
    "\n",
    "    def _normalize_vertices(self, vertices: np.ndarray) -> np.ndarray:\n",
    "        center = vertices.mean(axis=0)\n",
    "        vertices -= center\n",
    "        max_dist = np.max(np.linalg.norm(vertices, axis=1))\n",
    "        if max_dist > 0:\n",
    "            vertices /= max_dist\n",
    "        return vertices\n",
    "\n",
    "    def _resize_mesh(self, vertices: np.ndarray, faces: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        n_vertices = len(vertices)\n",
    "        n_faces = len(faces)\n",
    "        if n_vertices > self.max_vertices:\n",
    "            indices = np.linspace(0, n_vertices-1, self.max_vertices, dtype=int)\n",
    "            vertices = vertices[indices]\n",
    "            faces = self._remap_faces(faces, indices)\n",
    "        elif n_vertices < self.max_vertices:\n",
    "            padding = np.tile(vertices[-1:], (self.max_vertices - n_vertices, 1)) if n_vertices else np.zeros((self.max_vertices, 3), np.float32)\n",
    "            vertices = np.vstack([vertices, padding])\n",
    "        if len(faces) > self.max_faces:\n",
    "            indices = np.linspace(0, len(faces)-1, self.max_faces, dtype=int)\n",
    "            faces = faces[indices]\n",
    "        elif len(faces) < self.max_faces:\n",
    "            padding = np.tile(faces[-1:], (self.max_faces - len(faces), 1)) if len(faces) else np.zeros((self.max_faces, 3), dtype=np.int32)\n",
    "            faces = np.vstack([faces, padding])\n",
    "        return vertices, faces\n",
    "\n",
    "    def _remap_faces(self, faces: np.ndarray, vertex_indices: np.ndarray) -> np.ndarray:\n",
    "        old_to_new = {old: new for new, old in enumerate(vertex_indices)}\n",
    "        valid_faces = [[old_to_new[v] for v in face] for face in faces if all(v in old_to_new for v in face)]\n",
    "        return np.array(valid_faces, dtype=np.int32) if valid_faces else np.zeros((0, 3), dtype=np.int32)\n",
    "\n",
    "    def _compute_face_normals(self, vertices: np.ndarray, faces: np.ndarray) -> np.ndarray:\n",
    "        normals = []\n",
    "        for face in faces:\n",
    "            if len(np.unique(face)) == 3:\n",
    "                v0, v1, v2 = vertices[face]\n",
    "                normal = np.cross(v1 - v0, v2 - v0)\n",
    "                norm = np.linalg.norm(normal)\n",
    "                normals.append(normal / norm if norm > 1e-8 else [0.0, 0.0, 1.0])\n",
    "            else:\n",
    "                normals.append([0.0, 0.0, 1.0])\n",
    "        while len(normals) < self.max_faces:\n",
    "            normals.append([0.0, 0.0, 1.0])\n",
    "        return np.array(normals[:self.max_faces], dtype=np.float32)\n",
    "\n",
    "    def _compute_edge_features(self, vertices: np.ndarray, faces: np.ndarray) -> np.ndarray:\n",
    "        features = []\n",
    "        for face in faces:\n",
    "            if len(np.unique(face)) == 3:\n",
    "                v0, v1, v2 = vertices[face]\n",
    "                e1, e2, e3 = v1-v0, v2-v0, v2-v1\n",
    "                area = 0.5 * np.linalg.norm(np.cross(e1, e2))\n",
    "                peri = np.linalg.norm(e1) + np.linalg.norm(e2) + np.linalg.norm(e3)\n",
    "                lengths = [np.linalg.norm(e1), np.linalg.norm(e2), np.linalg.norm(e3)]\n",
    "                ar = max(lengths) / (min(lengths) + 1e-8)\n",
    "                angle = np.arccos(np.clip(np.dot(e1, e2)/(np.linalg.norm(e1)*np.linalg.norm(e2)+1e-8), -1.0, 1.0))\n",
    "                dist = np.linalg.norm((v0 + v1 + v2) / 3.0)\n",
    "                features.append([area, peri, ar, angle, dist])\n",
    "            else:\n",
    "                features.append([0.0, 0.0, 1.0, 0.0, 0.0])\n",
    "        while len(features) < self.max_faces:\n",
    "            features.append([0.0, 0.0, 1.0, 0.0, 0.0])\n",
    "        return np.array(features[:self.max_faces], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c05180-d45f-4dca-9592-092a9631ccae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeshDataManager:\n",
    "    def __init__(self, source_dir: str, training_dir: str, max_vertices: int = 5000, max_faces: int = 10000):\n",
    "        self.source_dir = Path(source_dir)\n",
    "        self.training_dir = Path(training_dir)\n",
    "        self.max_vertices = max_vertices\n",
    "        self.max_faces = max_faces\n",
    "\n",
    "        # 학습 데이터 디렉토리 구조 생성\n",
    "        self.train_dir = self.training_dir / \"train\"\n",
    "        self.val_dir = self.training_dir / \"val\"\n",
    "        self.test_dir = self.training_dir / \"test\"\n",
    "        self.processed_dir = self.training_dir / \"processed\"\n",
    "        self.cache_dir = self.training_dir / \"cache\"\n",
    "        self.analysis_dir = self.training_dir / \"analysis\"\n",
    "\n",
    "        for dir_path in [\n",
    "            self.training_dir, self.train_dir, self.val_dir,\n",
    "            self.test_dir, self.processed_dir, self.cache_dir, self.analysis_dir\n",
    "        ]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 추후 사용을 위한 파일 리스트 저장용 변수 초기화\n",
    "        self.train_files: List[str] = []\n",
    "        self.val_files: List[str] = []\n",
    "        self.test_files: List[str] = []\n",
    "\n",
    "    def prepare_training_data(self, train_ratio: float = 0.7, val_ratio: float = 0.2, test_ratio: float = 0.1):\n",
    "        \"\"\"원본 데이터를 학습/검증/테스트 세트로 분할하여 복사\"\"\"\n",
    "        print(\"📁 학습 데이터 준비 중...\")\n",
    "\n",
    "        loader = MeshLoader(self.source_dir)\n",
    "        all_files = loader.get_all_file_paths()\n",
    "\n",
    "        if len(all_files) == 0:\n",
    "            print(\"❌ 원본 디렉토리에서 메쉬 파일을 찾을 수 없습니다.\")\n",
    "            return False\n",
    "\n",
    "        print(f\"총 {len(all_files)}개의 메쉬 파일 발견\")\n",
    "\n",
    "        # 데이터 분할\n",
    "        train_files, temp_files = train_test_split(all_files, train_size=train_ratio, random_state=42)\n",
    "        val_files, test_files = train_test_split(temp_files, train_size=val_ratio / (val_ratio + test_ratio), random_state=42)\n",
    "\n",
    "        # 멤버 변수로 저장\n",
    "        self.train_files = train_files\n",
    "        self.val_files = val_files\n",
    "        self.test_files = test_files\n",
    "\n",
    "        # 파일 복사\n",
    "        datasets = [\n",
    "            (train_files, self.train_dir, \"train\"),\n",
    "            (val_files, self.val_dir, \"validation\"),\n",
    "            (test_files, self.test_dir, \"test\")\n",
    "        ]\n",
    "\n",
    "        for files, target_dir, name in datasets:\n",
    "            print(f\"{name} 세트: {len(files)}개 파일 복사 중...\")\n",
    "            for file_path in files:\n",
    "                source_file = Path(file_path)\n",
    "                target_file = target_dir / source_file.name\n",
    "                if not target_file.exists():\n",
    "                    shutil.copy2(source_file, target_file)\n",
    "\n",
    "        # 분할 정보 저장\n",
    "        split_info = {\n",
    "            'total_files': len(all_files),\n",
    "            'train_files': len(train_files),\n",
    "            'val_files': len(val_files),\n",
    "            'test_files': len(test_files),\n",
    "            'train_ratio': train_ratio,\n",
    "            'val_ratio': val_ratio,\n",
    "            'test_ratio': test_ratio\n",
    "        }\n",
    "\n",
    "        with open(self.training_dir / \"split_info.json\", 'w') as f:\n",
    "            json.dump(split_info, f, indent=2)\n",
    "\n",
    "        print(\"✅ 데이터 분할 완료!\")\n",
    "        print(f\"  - 학습: {len(train_files)}개\")\n",
    "        print(f\"  - 검증: {len(val_files)}개\")\n",
    "        print(f\"  - 테스트: {len(test_files)}개\")\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9acc1583-4e86-4c90-b370-53b1ecb8ac13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeshDatasetTF:\n",
    "    def __init__(self, data_dir: str, cache_dir: Optional[str] = None,\n",
    "                 max_vertices: int = 5000, max_faces: int = 10000):\n",
    "        self.loader = MeshLoader(data_dir)\n",
    "        self.preprocessor = MeshPreprocessor(max_vertices, max_faces)\n",
    "        self.max_vertices = max_vertices\n",
    "        self.max_faces = max_faces\n",
    "        self.cache_dir = Path(cache_dir or (Path(data_dir) / \"cache\"))\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.mesh_files = self.loader.get_all_file_paths()\n",
    "\n",
    "    def _load_and_process(self, file_path_tensor):\n",
    "        file_path = file_path_tensor.numpy().decode('utf-8')\n",
    "        cache_file = self.cache_dir / f\"{Path(file_path).stem}_{hash(file_path) % 10000}.pkl\"\n",
    "\n",
    "        if cache_file.exists():\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                mesh_data = pickle.load(f)\n",
    "        else:\n",
    "            raw = self.loader.load_mesh_data(file_path)\n",
    "            mesh_data = self.preprocessor.preprocess(raw)\n",
    "            mesh_data_np = {k: v.numpy() for k, v in mesh_data.items()}\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(mesh_data_np, f)\n",
    "\n",
    "        return tuple(tf.constant(mesh_data[k], dtype=tf.float32 if k != 'faces' else tf.int32)\n",
    "                     for k in ['vertices', 'faces', 'normals', 'features'])\n",
    "\n",
    "    def create_dataset(self, batch_size=8, shuffle=True, prefetch_buffer=tf.data.AUTOTUNE):\n",
    "        # 파일이 없는 경우 빈 데이터셋 반환\n",
    "        if len(self.mesh_files) == 0:\n",
    "            print(f\"⚠️ 데이터 디렉토리에 메쉬 파일이 없습니다: {self.loader.data_dir}\")\n",
    "            # 빈 데이터셋 생성\n",
    "            dummy_data = {\n",
    "                'vertices': tf.zeros([batch_size, self.max_vertices, 3], dtype=tf.float32),\n",
    "                'faces': tf.zeros([batch_size, self.max_faces, 3], dtype=tf.int32),\n",
    "                'normals': tf.zeros([batch_size, self.max_faces, 3], dtype=tf.float32),\n",
    "                'features': tf.zeros([batch_size, self.max_faces, 5], dtype=tf.float32)\n",
    "            }\n",
    "            return tf.data.Dataset.from_tensor_slices(dummy_data).take(0)\n",
    "        \n",
    "        ds = tf.data.Dataset.from_tensor_slices(self.mesh_files)\n",
    "        \n",
    "        # shuffle은 파일이 1개 이상일 때만 적용\n",
    "        if shuffle and len(self.mesh_files) > 1:\n",
    "            ds = ds.shuffle(len(self.mesh_files))\n",
    "        elif shuffle and len(self.mesh_files) == 1:\n",
    "            print(\"⚠️ 파일이 1개뿐이므로 shuffle을 건너뜁니다.\")\n",
    "            \n",
    "        ds = ds.map(lambda path: tf.py_function(\n",
    "            self._load_and_process, inp=[path],\n",
    "            Tout=[tf.float32, tf.int32, tf.float32, tf.float32]\n",
    "        ))\n",
    "        ds = ds.map(lambda v, f, n, feat: {\n",
    "            'vertices': tf.reshape(v, [self.max_vertices, 3]),\n",
    "            'faces': tf.reshape(f, [self.max_faces, 3]),\n",
    "            'normals': tf.reshape(n, [self.max_faces, 3]),\n",
    "            'features': tf.reshape(feat, [self.max_faces, 5])\n",
    "        })\n",
    "        return ds.batch(batch_size).prefetch(prefetch_buffer)\n",
    "\n",
    "def setup_environment(source_dir: str, training_dir: str, \n",
    "                             max_vertices: int = 2048, max_faces: int = 4096):\n",
    "    \"\"\"학습 환경 설정 및 데이터 준비\"\"\"\n",
    "    print(\"🚀 학습 환경 설정 시작\")\n",
    "    \n",
    "    # 1. 데이터 매니저 생성\n",
    "    data_manager = MeshDataManager(\n",
    "        source_dir=source_dir,\n",
    "        training_dir=training_dir,\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    )\n",
    "    \n",
    "    # 2. 학습 데이터 준비\n",
    "    if not data_manager.prepare_training_data():\n",
    "        return None\n",
    "    \n",
    "    # 3. 요약 정보 저장\n",
    "    summary = {\n",
    "        'setup_completed': True,\n",
    "        'source_dir': source_dir,\n",
    "        'training_dir': training_dir,\n",
    "        'max_vertices': max_vertices,\n",
    "        'max_faces': max_faces\n",
    "    }\n",
    "    \n",
    "    with open(data_manager.training_dir / \"setup_summary.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(\"✅ 학습 환경 설정 완료!\")\n",
    "    print(f\"📁 학습 데이터 디렉토리: {training_dir}\")\n",
    "    \n",
    "    return data_manager\n",
    "\n",
    "def create_training_datasets(training_dir: str, batch_size: int = 8, \n",
    "                           max_vertices: int = 5000, max_faces: int = 10000):\n",
    "    \"\"\"학습용 TensorFlow 데이터셋 생성\"\"\"\n",
    "    print(\"🔄 TensorFlow 데이터셋 생성 중...\")\n",
    "    \n",
    "    training_path = Path(training_dir)\n",
    "    cache_dir = training_path / \"cache\"\n",
    "    \n",
    "    # 각 디렉토리의 파일 개수 확인\n",
    "    train_files = list((training_path / \"train\").glob(\"*.*\"))\n",
    "    val_files = list((training_path / \"val\").glob(\"*.*\"))\n",
    "    test_files = list((training_path / \"test\").glob(\"*.*\"))\n",
    "    \n",
    "    print(f\"📊 데이터 현황:\")\n",
    "    print(f\"  - 학습: {len(train_files)}개 파일\")\n",
    "    print(f\"  - 검증: {len(val_files)}개 파일\")\n",
    "    print(f\"  - 테스트: {len(test_files)}개 파일\")\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"train\"),\n",
    "        cache_dir=str(cache_dir / \"train\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"val\"),\n",
    "        cache_dir=str(cache_dir / \"val\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    test_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"test\"),\n",
    "        cache_dir=str(cache_dir / \"test\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"✅ 데이터셋 생성 완료!\")\n",
    "    print(f\"  - 배치 크기: {batch_size}\")\n",
    "    print(f\"  - 최대 정점 수: {max_vertices}\")\n",
    "    print(f\"  - 최대 면 수: {max_faces}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23937a5-b3ad-4625-9f0c-2eb1e4b9145e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "특성 분석 및 시각화 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362dac72-834e-4be6-a4b7-01874995745f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 특성 분석 및 시각화 클래스\n",
    "# =============================================================================\n",
    "\n",
    "class MeshFeatureAnalyzer:\n",
    "    def __init__(self, dataset: MeshDatasetTF):\n",
    "        self.dataset = dataset\n",
    "        self.feature_columns = ['area', 'perimeter', 'aspect_ratio', 'angle', 'centroid_distance']\n",
    "        \n",
    "    def analyze_features(self, num_samples: int = 75) -> pd.DataFrame:\n",
    "        \"\"\"메쉬 파일들의 특성을 분석하여 DataFrame으로 반환\"\"\"\n",
    "        print(f\"📊 {num_samples}개 샘플의 특성을 분석중...\")\n",
    "        \n",
    "        all_features = []\n",
    "        loader = MeshLoader(self.dataset.source_dir)\n",
    "        file_paths = loader.get_all_file_paths()[:num_samples]\n",
    "        preprocessor = MeshPreprocessor(self.dataset.max_vertices, self.dataset.max_faces)\n",
    "\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"진행률: {i+1}/{len(file_paths)}\")\n",
    "                \n",
    "            try:\n",
    "                # 메쉬 데이터 로드 및 전처리\n",
    "                raw_data = self.dataset.loader.load_mesh_data(file_path)\n",
    "                processed_data = self.dataset.preprocessor.preprocess(raw_data)\n",
    "                \n",
    "                # 특성 추출 (TensorFlow tensor를 numpy로 변환)\n",
    "                features = processed_data['features'].numpy()\n",
    "                \n",
    "                # 각 face의 특성을 개별 행으로 저장\n",
    "                for face_idx, feature in enumerate(features):\n",
    "                    if not np.allclose(feature, [0.0, 0.0, 1.0, 0.0, 0.0]):  # 패딩된 face 제외\n",
    "                        feature_dict = {\n",
    "                            'file_path': Path(file_path).name,\n",
    "                            'face_idx': face_idx,\n",
    "                            'area': feature[0],\n",
    "                            'perimeter': feature[1], \n",
    "                            'aspect_ratio': feature[2],\n",
    "                            'angle': feature[3],\n",
    "                            'centroid_distance': feature[4]\n",
    "                        }\n",
    "                        all_features.append(feature_dict)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 특성 분석 실패 {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        df = pd.DataFrame(all_features)\n",
    "        print(f\"✅ 총 {len(df)}개의 face 특성을 분석했습니다.\")\n",
    "        return df\n",
    "    \n",
    "    def visualize_features(self, df: pd.DataFrame, save_plots: bool = True):\n",
    "        \"\"\"특성 데이터를 시각화\"\"\"\n",
    "        print(\"📈 특성 데이터 시각화 중...\")\n",
    "        \n",
    "        # 1. 기본 통계 정보\n",
    "        print(\"\\n=== 기본 통계 정보 ===\")\n",
    "        print(df[self.feature_columns].describe())\n",
    "        \n",
    "        # 2. 결측치 확인\n",
    "        print(\"\\n=== 결측치 확인 ===\")\n",
    "        missing_data = df[self.feature_columns].isnull().sum()\n",
    "        print(missing_data)\n",
    "        \n",
    "        # 3. 무한값 확인\n",
    "        print(\"\\n=== 무한값 확인 ===\")\n",
    "        for col in self.feature_columns:\n",
    "            inf_count = np.isinf(df[col]).sum()\n",
    "            print(f\"{col}: {inf_count}개의 무한값\")\n",
    "        \n",
    "        # 4. 시각화\n",
    "        plt.style.use('default')\n",
    "        \n",
    "        # 4-1. 히스토그램\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        fig.suptitle('메쉬 특성 분포 히스토그램', fontsize=16)\n",
    "        \n",
    "        for i, col in enumerate(self.feature_columns):\n",
    "            row, col_idx = divmod(i, 3)\n",
    "            \n",
    "            # 이상치 제거를 위한 percentile 기반 필터링\n",
    "            q1 = df[col].quantile(0.01)\n",
    "            q99 = df[col].quantile(0.99)\n",
    "            filtered_data = df[col][(df[col] >= q1) & (df[col] <= q99)]\n",
    "            \n",
    "            axes[row, col_idx].hist(filtered_data, bins=50, alpha=0.7, edgecolor='black')\n",
    "            axes[row, col_idx].set_title(f'{col}')\n",
    "            axes[row, col_idx].set_xlabel('값')\n",
    "            axes[row, col_idx].set_ylabel('빈도')\n",
    "            axes[row, col_idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 빈 subplot 제거\n",
    "        fig.delaxes(axes[1, 2])\n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            plt.savefig('mesh_feature_histograms.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # 4-2. 박스플롯 (이상치 탐지)\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 6))\n",
    "        fig.suptitle('메쉬 특성 박스플롯 (이상치 탐지)', fontsize=16)\n",
    "        \n",
    "        for i, col in enumerate(self.feature_columns):\n",
    "            # 로그 스케일 적용 (양수 값만)\n",
    "            positive_data = df[col][df[col] > 0]\n",
    "            if len(positive_data) > 0:\n",
    "                axes[i].boxplot(positive_data, vert=True)\n",
    "                axes[i].set_yscale('log')\n",
    "            else:\n",
    "                axes[i].boxplot(df[col], vert=True)\n",
    "            \n",
    "            axes[i].set_title(f'{col}')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            plt.savefig('mesh_feature_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # 4-3. 상관관계 히트맵\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        correlation_matrix = df[self.feature_columns].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, fmt='.3f')\n",
    "        plt.title('메쉬 특성 간 상관관계')\n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            plt.savefig('mesh_feature_correlation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # 4-4. 산점도 (주요 특성 간 관계)\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        fig.suptitle('주요 특성 간 관계', fontsize=16)\n",
    "        \n",
    "        # area vs perimeter\n",
    "        axes[0,0].scatter(df['area'], df['perimeter'], alpha=0.5, s=1)\n",
    "        axes[0,0].set_xlabel('Area')\n",
    "        axes[0,0].set_ylabel('Perimeter')\n",
    "        axes[0,0].set_title('Area vs Perimeter')\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # aspect_ratio vs angle\n",
    "        axes[0,1].scatter(df['aspect_ratio'], df['angle'], alpha=0.5, s=1)\n",
    "        axes[0,1].set_xlabel('Aspect Ratio')\n",
    "        axes[0,1].set_ylabel('Angle')\n",
    "        axes[0,1].set_title('Aspect Ratio vs Angle')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # area vs centroid_distance\n",
    "        axes[1,0].scatter(df['area'], df['centroid_distance'], alpha=0.5, s=1)\n",
    "        axes[1,0].set_xlabel('Area')\n",
    "        axes[1,0].set_ylabel('Centroid Distance')\n",
    "        axes[1,0].set_title('Area vs Centroid Distance')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # perimeter vs aspect_ratio\n",
    "        axes[1,1].scatter(df['perimeter'], df['aspect_ratio'], alpha=0.5, s=1)\n",
    "        axes[1,1].set_xlabel('Perimeter')\n",
    "        axes[1,1].set_ylabel('Aspect Ratio')\n",
    "        axes[1,1].set_title('Perimeter vs Aspect Ratio')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            plt.savefig('mesh_feature_scatter.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def detect_anomalies(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"이상치 탐지\"\"\"\n",
    "        print(\"\\n🔍 이상치 탐지 중...\")\n",
    "        \n",
    "        anomalies = []\n",
    "        \n",
    "        for col in self.feature_columns:\n",
    "            # IQR 방법\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            \n",
    "            print(f\"{col}: {len(outliers)}개의 이상치 ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "            \n",
    "            # 이상치 정보 저장\n",
    "            for idx, row in outliers.iterrows():\n",
    "                anomalies.append({\n",
    "                    'file_path': row['file_path'],\n",
    "                    'face_idx': row['face_idx'],\n",
    "                    'feature': col,\n",
    "                    'value': row[col],\n",
    "                    'lower_bound': lower_bound,\n",
    "                    'upper_bound': upper_bound\n",
    "                })\n",
    "        \n",
    "        anomaly_df = pd.DataFrame(anomalies)\n",
    "        return anomaly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ecfe2d5-10fb-4078-a808-b3ec26679f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 실행 함수\n",
    "# =============================================================================\n",
    "\n",
    "def setup_environment(source_dir: str, training_dir: str, \n",
    "                             max_vertices: int = 2048, max_faces: int = 4096):\n",
    "    \"\"\"학습 환경 설정 및 데이터 준비\"\"\"\n",
    "    print(\"🚀 학습 환경 설정 시작\")\n",
    "    \n",
    "    # 1. 데이터 매니저 생성\n",
    "    data_manager = MeshDataManager(\n",
    "        source_dir=source_dir,\n",
    "        training_dir=training_dir,\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    )\n",
    "    \n",
    "    # 2. 학습 데이터 준비\n",
    "    if not data_manager.prepare_training_data():\n",
    "        return None\n",
    "    \n",
    "    # 3. 특성 분석\n",
    "    analyzer = MeshFeatureAnalyzer(data_manager)\n",
    "    feature_results = analyzer.analyze_features(num_samples=7)\n",
    "    \n",
    "    # 4. 시각화\n",
    "    analyzer.visualize_features(feature_results)\n",
    "    \n",
    "    # 5. 요약 정보 저장\n",
    "    summary = {\n",
    "        'setup_completed': True,\n",
    "        'source_dir': source_dir,\n",
    "        'training_dir': training_dir,\n",
    "        'max_vertices': max_vertices,\n",
    "        'max_faces': max_faces,\n",
    "        'datasets_analyzed': list(feature_results.keys()),\n",
    "        'total_features_analyzed': sum(len(df) for df in feature_results.values())\n",
    "    }\n",
    "    \n",
    "    with open(data_manager.training_dir / \"setup_summary.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(\"✅ 학습 환경 설정 완료!\")\n",
    "    print(f\"📁 학습 데이터 디렉토리: {training_dir}\")\n",
    "    print(f\"📊 분석 결과: {data_manager.analysis_dir}\")\n",
    "    \n",
    "    return data_manager\n",
    "\n",
    "def create_training_datasets(training_dir: str, batch_size: int = 8, \n",
    "                           max_vertices: int = 5000, max_faces: int = 10000):\n",
    "    \"\"\"학습용 TensorFlow 데이터셋 생성\"\"\"\n",
    "    print(\"🔄 TensorFlow 데이터셋 생성 중...\")\n",
    "    \n",
    "    training_path = Path(training_dir)\n",
    "    cache_dir = training_path / \"cache\"\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"train\"),\n",
    "        cache_dir=str(cache_dir / \"train\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"val\"),\n",
    "        cache_dir=str(cache_dir / \"val\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    test_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"test\"),\n",
    "        cache_dir=str(cache_dir / \"test\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"✅ 데이터셋 생성 완료!\")\n",
    "    print(f\"  - 배치 크기: {batch_size}\")\n",
    "    print(f\"  - 최대 정점 수: {max_vertices}\")\n",
    "    print(f\"  - 최대 면 수: {max_faces}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac441b1-a88d-41d5-9011-c1b85f0e87b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "메쉬 데이터 학습 환경 설정\n",
      "============================================================\n",
      "🔧 하드웨어 설정: 정점=2048, 면=4096, 배치=8\n",
      "🚀 학습 환경 설정 시작\n",
      "📁 학습 데이터 준비 중...\n",
      "총 75개의 메쉬 파일 발견\n",
      "train 세트: 52개 파일 복사 중...\n",
      "validation 세트: 15개 파일 복사 중...\n",
      "test 세트: 8개 파일 복사 중...\n",
      "✅ 데이터 분할 완료!\n",
      "  - 학습: 52개\n",
      "  - 검증: 15개\n",
      "  - 테스트: 8개\n",
      "📊 7개 샘플의 특성을 분석중...\n",
      "진행률: 1/7\n",
      "⚠️ 특성 분석 실패 C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\결절 모델링 npy\\A0083.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "⚠️ 특성 분석 실패 C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\결절 모델링 npy\\A0087.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "⚠️ 특성 분석 실패 C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\결절 모델링 npy\\A0103.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "⚠️ 특성 분석 실패 C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\결절 모델링 npy\\A0106.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "⚠️ 특성 분석 실패 C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\결절 모델링 npy\\A0117.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "⚠️ 특성 분석 실패 C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\결절 모델링 npy\\A0126.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "⚠️ 특성 분석 실패 C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\결절 모델링 npy\\A0165.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "✅ 총 0개의 face 특성을 분석했습니다.\n",
      "📈 특성 데이터 시각화 중...\n",
      "\n",
      "=== 기본 통계 정보 ===\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['area', 'perimeter', 'aspect_ratio', 'angle', 'centroid_distance'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔧 하드웨어 설정: 정점=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_VERTICES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 면=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_FACES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 배치=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 1. 학습 환경 설정\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m data_manager \u001b[38;5;241m=\u001b[39m \u001b[43msetup_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSOURCE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAINING_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_vertices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_VERTICES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_faces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_FACES\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ 학습 환경 설정 실패\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m, in \u001b[0;36msetup_environment\u001b[1;34m(source_dir, training_dir, max_vertices, max_faces)\u001b[0m\n\u001b[0;32m     24\u001b[0m feature_results \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39manalyze_features(num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 4. 시각화\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 5. 요약 정보 저장\u001b[39;00m\n\u001b[0;32m     30\u001b[0m summary \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetup_completed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_dir\u001b[39m\u001b[38;5;124m'\u001b[39m: source_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_features_analyzed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m feature_results\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     38\u001b[0m }\n",
      "Cell \u001b[1;32mIn[6], line 59\u001b[0m, in \u001b[0;36mMeshFeatureAnalyzer.visualize_features\u001b[1;34m(self, df, save_plots)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 1. 기본 통계 정보\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== 기본 통계 정보 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe())\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# 2. 결측치 확인\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== 결측치 확인 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\project2025\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\project2025\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\project2025\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['area', 'perimeter', 'aspect_ratio', 'angle', 'centroid_distance'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 메인 실행 부분\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    BASE_DIR = \"C:/Users/konyang/Desktop/project2025/model/data/npy\"\n",
    "    SOURCE_DIR = os.path.join(BASE_DIR, \"결절 모델링 npy\")  # 원본 메쉬 파일 디렉토리\n",
    "    TRAINING_DIR = \"C:/Users/konyang/Desktop/MeshCNN_TF/data/train\"      # 학습 데이터 저장 디렉토리\n",
    "    \n",
    "    # 학습 설정 - 환경에 맞게 조정하세요\n",
    "    '''\n",
    "    고성능 GPU (24GB+): MAX_VERTICES=5000, MAX_FACES=10000, BATCH_SIZE=32\n",
    "    중간 GPU (8-16GB): MAX_VERTICES=3000, MAX_FACES=6000, BATCH_SIZE=16  \n",
    "    저사양 GPU (4-8GB): MAX_VERTICES=1500, MAX_FACES=3000, BATCH_SIZE=8\n",
    "    '''\n",
    "    MAX_VERTICES = 2048\n",
    "    MAX_FACES = 4096\n",
    "    BATCH_SIZE = 8\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"메쉬 데이터 학습 환경 설정\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"🔧 하드웨어 설정: 정점={MAX_VERTICES}, 면={MAX_FACES}, 배치={BATCH_SIZE}\")\n",
    "    \n",
    "    # 1. 학습 환경 설정\n",
    "    data_manager = setup_environment(\n",
    "        source_dir=SOURCE_DIR,\n",
    "        training_dir=TRAINING_DIR,\n",
    "        max_vertices=MAX_VERTICES,\n",
    "        max_faces=MAX_FACES\n",
    "    )\n",
    "    \n",
    "    if data_manager is None:\n",
    "        print(\"❌ 학습 환경 설정 실패\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 2. 학습용 데이터셋 생성\n",
    "    train_ds, val_ds, test_ds = create_datasets(\n",
    "        training_dir=TRAINING_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_vertices=MAX_VERTICES,\n",
    "        max_faces=MAX_FACES\n",
    "    )\n",
    "    \n",
    "    # 3. 데이터셋 확인\n",
    "    print(\"\\n🔍 데이터셋 확인:\")\n",
    "    for batch in train_ds.take(1):\n",
    "        print(f\"Batch shape:\")\n",
    "        for key, value in batch.items():\n",
    "            print(f\"  {key}: {value.shape}\")\n",
    "    \n",
    "    print(\"\\n🎉 모든 준비 완료! 이제 학습을 시작할 수 있습니다.\")\n",
    "    print(f\"📂 학습 데이터: {TRAINING_DIR}\")\n",
    "    print(f\"📊 분석 결과: {TRAINING_DIR}/analysis/\")\n",
    "    print(f\"💡 설정을 변경하려면 위의 MAX_VERTICES, MAX_FACES, BATCH_SIZE 값을 조정하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85826a3-18d9-44db-bc14-647124037823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2025",
   "language": "python",
   "name": "project2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
