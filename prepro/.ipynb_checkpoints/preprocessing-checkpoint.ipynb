{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1c5d18-2e37-44ac-a5e9-4b0d49613d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05effe5f-21bb-440b-b530-dec4907b47e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ë©”ì‰¬ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì—­í• \n",
    "# =============================================================================\n",
    "\n",
    "class MeshLoader:\n",
    "    def __init__(self, data_dir: str, supported_formats=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.supported_formats = supported_formats or ['.npy', '.obj', '.ply', '.stl', '.off']\n",
    "        self.mesh_files = self._collect_mesh_files()\n",
    "\n",
    "    def _collect_mesh_files(self) -> List[Path]:\n",
    "        files = []\n",
    "        for ext in self.supported_formats:\n",
    "            files.extend(self.data_dir.glob(f\"**/*{ext}\"))\n",
    "        return sorted(files)\n",
    "\n",
    "    def get_all_file_paths(self) -> List[str]:\n",
    "        return [str(f) for f in self.mesh_files]\n",
    "\n",
    "    def load_mesh_data(self, file_path: str) -> Dict:\n",
    "        try:\n",
    "            mesh = trimesh.load(file_path, force='mesh')\n",
    "            if hasattr(mesh, 'geometry'):\n",
    "                mesh = list(mesh.geometry.values())[0]\n",
    "            vertices = np.array(mesh.vertices, dtype=np.float32)\n",
    "            faces = np.array(mesh.faces, dtype=np.int32)\n",
    "            if len(vertices) == 0 or len(faces) == 0:\n",
    "                raise ValueError(\"ë¹ˆ ë©”ì‰¬\")\n",
    "            return {'vertices': vertices, 'faces': faces, 'file_path': file_path}\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ë¡œë”© ì‹¤íŒ¨ {file_path}: {e}\")\n",
    "            return self._create_dummy_mesh()\n",
    "\n",
    "    def _create_dummy_mesh(self) -> Dict:\n",
    "        vertices = np.array([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [1.0, 0.0, 0.0],\n",
    "            [0.5, 0.866, 0.0],\n",
    "            [0.5, 0.289, 0.816]\n",
    "        ], dtype=np.float32)\n",
    "        faces = np.array([\n",
    "            [0, 1, 2],\n",
    "            [0, 2, 3],\n",
    "            [0, 3, 1],\n",
    "            [1, 3, 2]\n",
    "        ], dtype=np.int32)\n",
    "        return {'vertices': vertices, 'faces': faces, 'file_path': 'dummy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c70f04-3483-4317-92d4-1bc36b8c1f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeshPreprocessor:\n",
    "    def __init__(self, max_vertices: int, max_faces: int):\n",
    "        self.max_vertices = max_vertices\n",
    "        self.max_faces = max_faces\n",
    "\n",
    "    def preprocess(self, mesh_data: Dict) -> Dict:\n",
    "        vertices = self._normalize_vertices(mesh_data['vertices'])\n",
    "        vertices, faces = self._resize_mesh(vertices, mesh_data['faces'])\n",
    "        normals = self._compute_face_normals(vertices, faces)\n",
    "        features = self._compute_edge_features(vertices, faces)\n",
    "\n",
    "        return {\n",
    "            'vertices': tf.constant(vertices, dtype=tf.float32),\n",
    "            'faces': tf.constant(faces, dtype=tf.int32),\n",
    "            'normals': tf.constant(normals, dtype=tf.float32),\n",
    "            'features': tf.constant(features, dtype=tf.float32)\n",
    "        }\n",
    "\n",
    "    def _normalize_vertices(self, vertices: np.ndarray) -> np.ndarray:\n",
    "        center = vertices.mean(axis=0)\n",
    "        vertices -= center\n",
    "        max_dist = np.max(np.linalg.norm(vertices, axis=1))\n",
    "        if max_dist > 0:\n",
    "            vertices /= max_dist\n",
    "        return vertices\n",
    "\n",
    "    def _resize_mesh(self, vertices: np.ndarray, faces: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        n_vertices = len(vertices)\n",
    "        n_faces = len(faces)\n",
    "        if n_vertices > self.max_vertices:\n",
    "            indices = np.linspace(0, n_vertices-1, self.max_vertices, dtype=int)\n",
    "            vertices = vertices[indices]\n",
    "            faces = self._remap_faces(faces, indices)\n",
    "        elif n_vertices < self.max_vertices:\n",
    "            padding = np.tile(vertices[-1:], (self.max_vertices - n_vertices, 1)) if n_vertices else np.zeros((self.max_vertices, 3), np.float32)\n",
    "            vertices = np.vstack([vertices, padding])\n",
    "        if len(faces) > self.max_faces:\n",
    "            indices = np.linspace(0, len(faces)-1, self.max_faces, dtype=int)\n",
    "            faces = faces[indices]\n",
    "        elif len(faces) < self.max_faces:\n",
    "            padding = np.tile(faces[-1:], (self.max_faces - len(faces), 1)) if len(faces) else np.zeros((self.max_faces, 3), dtype=np.int32)\n",
    "            faces = np.vstack([faces, padding])\n",
    "        return vertices, faces\n",
    "\n",
    "    def _remap_faces(self, faces: np.ndarray, vertex_indices: np.ndarray) -> np.ndarray:\n",
    "        old_to_new = {old: new for new, old in enumerate(vertex_indices)}\n",
    "        valid_faces = [[old_to_new[v] for v in face] for face in faces if all(v in old_to_new for v in face)]\n",
    "        return np.array(valid_faces, dtype=np.int32) if valid_faces else np.zeros((0, 3), dtype=np.int32)\n",
    "\n",
    "    def _compute_face_normals(self, vertices: np.ndarray, faces: np.ndarray) -> np.ndarray:\n",
    "        normals = []\n",
    "        for face in faces:\n",
    "            if len(np.unique(face)) == 3:\n",
    "                v0, v1, v2 = vertices[face]\n",
    "                normal = np.cross(v1 - v0, v2 - v0)\n",
    "                norm = np.linalg.norm(normal)\n",
    "                normals.append(normal / norm if norm > 1e-8 else [0.0, 0.0, 1.0])\n",
    "            else:\n",
    "                normals.append([0.0, 0.0, 1.0])\n",
    "        while len(normals) < self.max_faces:\n",
    "            normals.append([0.0, 0.0, 1.0])\n",
    "        return np.array(normals[:self.max_faces], dtype=np.float32)\n",
    "\n",
    "    def _compute_edge_features(self, vertices: np.ndarray, faces: np.ndarray) -> np.ndarray:\n",
    "        features = []\n",
    "        for face in faces:\n",
    "            if len(np.unique(face)) == 3:\n",
    "                v0, v1, v2 = vertices[face]\n",
    "                e1, e2, e3 = v1-v0, v2-v0, v2-v1\n",
    "                area = 0.5 * np.linalg.norm(np.cross(e1, e2))\n",
    "                peri = np.linalg.norm(e1) + np.linalg.norm(e2) + np.linalg.norm(e3)\n",
    "                lengths = [np.linalg.norm(e1), np.linalg.norm(e2), np.linalg.norm(e3)]\n",
    "                ar = max(lengths) / (min(lengths) + 1e-8)\n",
    "                angle = np.arccos(np.clip(np.dot(e1, e2)/(np.linalg.norm(e1)*np.linalg.norm(e2)+1e-8), -1.0, 1.0))\n",
    "                dist = np.linalg.norm((v0 + v1 + v2) / 3.0)\n",
    "                features.append([area, peri, ar, angle, dist])\n",
    "            else:\n",
    "                features.append([0.0, 0.0, 1.0, 0.0, 0.0])\n",
    "        while len(features) < self.max_faces:\n",
    "            features.append([0.0, 0.0, 1.0, 0.0, 0.0])\n",
    "        return np.array(features[:self.max_faces], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c05180-d45f-4dca-9592-092a9631ccae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeshDataManager:\n",
    "    def __init__(self, source_dir: str, training_dir: str, max_vertices: int = 5000, max_faces: int = 10000):\n",
    "        self.source_dir = Path(source_dir)\n",
    "        self.training_dir = Path(training_dir)\n",
    "        self.max_vertices = max_vertices\n",
    "        self.max_faces = max_faces\n",
    "\n",
    "        # í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±\n",
    "        self.train_dir = self.training_dir / \"train\"\n",
    "        self.val_dir = self.training_dir / \"val\"\n",
    "        self.test_dir = self.training_dir / \"test\"\n",
    "        self.processed_dir = self.training_dir / \"processed\"\n",
    "        self.cache_dir = self.training_dir / \"cache\"\n",
    "        self.analysis_dir = self.training_dir / \"analysis\"\n",
    "\n",
    "        for dir_path in [\n",
    "            self.training_dir, self.train_dir, self.val_dir,\n",
    "            self.test_dir, self.processed_dir, self.cache_dir, self.analysis_dir\n",
    "        ]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # ì¶”í›„ ì‚¬ìš©ì„ ìœ„í•œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì €ì¥ìš© ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "        self.train_files: List[str] = []\n",
    "        self.val_files: List[str] = []\n",
    "        self.test_files: List[str] = []\n",
    "\n",
    "    def prepare_training_data(self, train_ratio: float = 0.7, val_ratio: float = 0.2, test_ratio: float = 0.1):\n",
    "        \"\"\"ì›ë³¸ ë°ì´í„°ë¥¼ í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•˜ì—¬ ë³µì‚¬\"\"\"\n",
    "        print(\"ğŸ“ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "\n",
    "        loader = MeshLoader(self.source_dir)\n",
    "        all_files = loader.get_all_file_paths()\n",
    "\n",
    "        if len(all_files) == 0:\n",
    "            print(\"âŒ ì›ë³¸ ë””ë ‰í† ë¦¬ì—ì„œ ë©”ì‰¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return False\n",
    "\n",
    "        print(f\"ì´ {len(all_files)}ê°œì˜ ë©”ì‰¬ íŒŒì¼ ë°œê²¬\")\n",
    "\n",
    "        # ë°ì´í„° ë¶„í• \n",
    "        train_files, temp_files = train_test_split(all_files, train_size=train_ratio, random_state=42)\n",
    "        val_files, test_files = train_test_split(temp_files, train_size=val_ratio / (val_ratio + test_ratio), random_state=42)\n",
    "\n",
    "        # ë©¤ë²„ ë³€ìˆ˜ë¡œ ì €ì¥\n",
    "        self.train_files = train_files\n",
    "        self.val_files = val_files\n",
    "        self.test_files = test_files\n",
    "\n",
    "        # íŒŒì¼ ë³µì‚¬\n",
    "        datasets = [\n",
    "            (train_files, self.train_dir, \"train\"),\n",
    "            (val_files, self.val_dir, \"validation\"),\n",
    "            (test_files, self.test_dir, \"test\")\n",
    "        ]\n",
    "\n",
    "        for files, target_dir, name in datasets:\n",
    "            print(f\"{name} ì„¸íŠ¸: {len(files)}ê°œ íŒŒì¼ ë³µì‚¬ ì¤‘...\")\n",
    "            for file_path in files:\n",
    "                source_file = Path(file_path)\n",
    "                target_file = target_dir / source_file.name\n",
    "                if not target_file.exists():\n",
    "                    shutil.copy2(source_file, target_file)\n",
    "\n",
    "        # ë¶„í•  ì •ë³´ ì €ì¥\n",
    "        split_info = {\n",
    "            'total_files': len(all_files),\n",
    "            'train_files': len(train_files),\n",
    "            'val_files': len(val_files),\n",
    "            'test_files': len(test_files),\n",
    "            'train_ratio': train_ratio,\n",
    "            'val_ratio': val_ratio,\n",
    "            'test_ratio': test_ratio\n",
    "        }\n",
    "\n",
    "        with open(self.training_dir / \"split_info.json\", 'w') as f:\n",
    "            json.dump(split_info, f, indent=2)\n",
    "\n",
    "        print(\"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ!\")\n",
    "        print(f\"  - í•™ìŠµ: {len(train_files)}ê°œ\")\n",
    "        print(f\"  - ê²€ì¦: {len(val_files)}ê°œ\")\n",
    "        print(f\"  - í…ŒìŠ¤íŠ¸: {len(test_files)}ê°œ\")\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9acc1583-4e86-4c90-b370-53b1ecb8ac13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeshDatasetTF:\n",
    "    def __init__(self, data_dir: str, cache_dir: Optional[str] = None,\n",
    "                 max_vertices: int = 5000, max_faces: int = 10000):\n",
    "        self.loader = MeshLoader(data_dir)\n",
    "        self.preprocessor = MeshPreprocessor(max_vertices, max_faces)\n",
    "        self.max_vertices = max_vertices\n",
    "        self.max_faces = max_faces\n",
    "        self.cache_dir = Path(cache_dir or (Path(data_dir) / \"cache\"))\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.mesh_files = self.loader.get_all_file_paths()\n",
    "\n",
    "    def _load_and_process(self, file_path_tensor):\n",
    "        file_path = file_path_tensor.numpy().decode('utf-8')\n",
    "        cache_file = self.cache_dir / f\"{Path(file_path).stem}_{hash(file_path) % 10000}.pkl\"\n",
    "\n",
    "        if cache_file.exists():\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                mesh_data = pickle.load(f)\n",
    "        else:\n",
    "            raw = self.loader.load_mesh_data(file_path)\n",
    "            mesh_data = self.preprocessor.preprocess(raw)\n",
    "            mesh_data_np = {k: v.numpy() for k, v in mesh_data.items()}\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(mesh_data_np, f)\n",
    "\n",
    "        return tuple(tf.constant(mesh_data[k], dtype=tf.float32 if k != 'faces' else tf.int32)\n",
    "                     for k in ['vertices', 'faces', 'normals', 'features'])\n",
    "\n",
    "    def create_dataset(self, batch_size=8, shuffle=True, prefetch_buffer=tf.data.AUTOTUNE):\n",
    "        # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë°ì´í„°ì…‹ ë°˜í™˜\n",
    "        if len(self.mesh_files) == 0:\n",
    "            print(f\"âš ï¸ ë°ì´í„° ë””ë ‰í† ë¦¬ì— ë©”ì‰¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.loader.data_dir}\")\n",
    "            # ë¹ˆ ë°ì´í„°ì…‹ ìƒì„±\n",
    "            dummy_data = {\n",
    "                'vertices': tf.zeros([batch_size, self.max_vertices, 3], dtype=tf.float32),\n",
    "                'faces': tf.zeros([batch_size, self.max_faces, 3], dtype=tf.int32),\n",
    "                'normals': tf.zeros([batch_size, self.max_faces, 3], dtype=tf.float32),\n",
    "                'features': tf.zeros([batch_size, self.max_faces, 5], dtype=tf.float32)\n",
    "            }\n",
    "            return tf.data.Dataset.from_tensor_slices(dummy_data).take(0)\n",
    "        \n",
    "        ds = tf.data.Dataset.from_tensor_slices(self.mesh_files)\n",
    "        \n",
    "        # shuffleì€ íŒŒì¼ì´ 1ê°œ ì´ìƒì¼ ë•Œë§Œ ì ìš©\n",
    "        if shuffle and len(self.mesh_files) > 1:\n",
    "            ds = ds.shuffle(len(self.mesh_files))\n",
    "        elif shuffle and len(self.mesh_files) == 1:\n",
    "            print(\"âš ï¸ íŒŒì¼ì´ 1ê°œë¿ì´ë¯€ë¡œ shuffleì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            \n",
    "        ds = ds.map(lambda path: tf.py_function(\n",
    "            self._load_and_process, inp=[path],\n",
    "            Tout=[tf.float32, tf.int32, tf.float32, tf.float32]\n",
    "        ))\n",
    "        ds = ds.map(lambda v, f, n, feat: {\n",
    "            'vertices': tf.reshape(v, [self.max_vertices, 3]),\n",
    "            'faces': tf.reshape(f, [self.max_faces, 3]),\n",
    "            'normals': tf.reshape(n, [self.max_faces, 3]),\n",
    "            'features': tf.reshape(feat, [self.max_faces, 5])\n",
    "        })\n",
    "        return ds.batch(batch_size).prefetch(prefetch_buffer)\n",
    "\n",
    "def setup_environment(source_dir: str, training_dir: str, \n",
    "                             max_vertices: int = 2048, max_faces: int = 4096):\n",
    "    \"\"\"í•™ìŠµ í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„\"\"\"\n",
    "    print(\"ğŸš€ í•™ìŠµ í™˜ê²½ ì„¤ì • ì‹œì‘\")\n",
    "    \n",
    "    # 1. ë°ì´í„° ë§¤ë‹ˆì € ìƒì„±\n",
    "    data_manager = MeshDataManager(\n",
    "        source_dir=source_dir,\n",
    "        training_dir=training_dir,\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    )\n",
    "    \n",
    "    # 2. í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "    if not data_manager.prepare_training_data():\n",
    "        return None\n",
    "    \n",
    "    # 3. ìš”ì•½ ì •ë³´ ì €ì¥\n",
    "    summary = {\n",
    "        'setup_completed': True,\n",
    "        'source_dir': source_dir,\n",
    "        'training_dir': training_dir,\n",
    "        'max_vertices': max_vertices,\n",
    "        'max_faces': max_faces\n",
    "    }\n",
    "    \n",
    "    with open(data_manager.training_dir / \"setup_summary.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(\"âœ… í•™ìŠµ í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬: {training_dir}\")\n",
    "    \n",
    "    return data_manager\n",
    "\n",
    "def create_training_datasets(training_dir: str, batch_size: int = 8, \n",
    "                           max_vertices: int = 5000, max_faces: int = 10000):\n",
    "    \"\"\"í•™ìŠµìš© TensorFlow ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    print(\"ğŸ”„ TensorFlow ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    training_path = Path(training_dir)\n",
    "    cache_dir = training_path / \"cache\"\n",
    "    \n",
    "    # ê° ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ê°œìˆ˜ í™•ì¸\n",
    "    train_files = list((training_path / \"train\").glob(\"*.*\"))\n",
    "    val_files = list((training_path / \"val\").glob(\"*.*\"))\n",
    "    test_files = list((training_path / \"test\").glob(\"*.*\"))\n",
    "    \n",
    "    print(f\"ğŸ“Š ë°ì´í„° í˜„í™©:\")\n",
    "    print(f\"  - í•™ìŠµ: {len(train_files)}ê°œ íŒŒì¼\")\n",
    "    print(f\"  - ê²€ì¦: {len(val_files)}ê°œ íŒŒì¼\")\n",
    "    print(f\"  - í…ŒìŠ¤íŠ¸: {len(test_files)}ê°œ íŒŒì¼\")\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ìƒì„±\n",
    "    train_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"train\"),\n",
    "        cache_dir=str(cache_dir / \"train\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"val\"),\n",
    "        cache_dir=str(cache_dir / \"val\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    test_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"test\"),\n",
    "        cache_dir=str(cache_dir / \"test\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!\")\n",
    "    print(f\"  - ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "    print(f\"  - ìµœëŒ€ ì •ì  ìˆ˜: {max_vertices}\")\n",
    "    print(f\"  - ìµœëŒ€ ë©´ ìˆ˜: {max_faces}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23937a5-b3ad-4625-9f0c-2eb1e4b9145e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "íŠ¹ì„± ë¶„ì„ ë° ì‹œê°í™” í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362dac72-834e-4be6-a4b7-01874995745f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# íŠ¹ì„± ë¶„ì„ ë° ì‹œê°í™” í´ë˜ìŠ¤\n",
    "# =============================================================================\n",
    "\n",
    "class MeshFeatureAnalyzer:\n",
    "    def __init__(self, dataset: MeshDatasetTF):\n",
    "        self.dataset = dataset\n",
    "        self.feature_columns = ['area', 'perimeter', 'aspect_ratio', 'angle', 'centroid_distance']\n",
    "        \n",
    "    def analyze_features(self, num_samples: int = 75) -> pd.DataFrame:\n",
    "        \"\"\"ë©”ì‰¬ íŒŒì¼ë“¤ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜\"\"\"\n",
    "        print(f\"ğŸ“Š {num_samples}ê°œ ìƒ˜í”Œì˜ íŠ¹ì„±ì„ ë¶„ì„ì¤‘...\")\n",
    "        \n",
    "        all_features = []\n",
    "        loader = MeshLoader(self.dataset.source_dir)\n",
    "        file_paths = loader.get_all_file_paths()[:num_samples]\n",
    "        preprocessor = MeshPreprocessor(self.dataset.max_vertices, self.dataset.max_faces)\n",
    "\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"ì§„í–‰ë¥ : {i+1}/{len(file_paths)}\")\n",
    "                \n",
    "            try:\n",
    "                # ë©”ì‰¬ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "                raw_data = self.dataset.loader.load_mesh_data(file_path)\n",
    "                processed_data = self.dataset.preprocessor.preprocess(raw_data)\n",
    "                \n",
    "                # íŠ¹ì„± ì¶”ì¶œ (TensorFlow tensorë¥¼ numpyë¡œ ë³€í™˜)\n",
    "                features = processed_data['features'].numpy()\n",
    "                \n",
    "                # ê° faceì˜ íŠ¹ì„±ì„ ê°œë³„ í–‰ìœ¼ë¡œ ì €ì¥\n",
    "                for face_idx, feature in enumerate(features):\n",
    "                    if not np.allclose(feature, [0.0, 0.0, 1.0, 0.0, 0.0]):  # íŒ¨ë”©ëœ face ì œì™¸\n",
    "                        feature_dict = {\n",
    "                            'file_path': Path(file_path).name,\n",
    "                            'face_idx': face_idx,\n",
    "                            'area': feature[0],\n",
    "                            'perimeter': feature[1], \n",
    "                            'aspect_ratio': feature[2],\n",
    "                            'angle': feature[3],\n",
    "                            'centroid_distance': feature[4]\n",
    "                        }\n",
    "                        all_features.append(feature_dict)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        df = pd.DataFrame(all_features)\n",
    "        print(f\"âœ… ì´ {len(df)}ê°œì˜ face íŠ¹ì„±ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return df\n",
    "    \n",
    "    def visualize_features(self, df: pd.DataFrame, save_plots: bool = True):\n",
    "        \"\"\"íŠ¹ì„± ë°ì´í„°ë¥¼ ì‹œê°í™”\"\"\"\n",
    "        print(\"ğŸ“ˆ íŠ¹ì„± ë°ì´í„° ì‹œê°í™” ì¤‘...\")\n",
    "        \n",
    "        # 1. ê¸°ë³¸ í†µê³„ ì •ë³´\n",
    "        print(\"\\n=== ê¸°ë³¸ í†µê³„ ì •ë³´ ===\")\n",
    "        print(df[self.feature_columns].describe())\n",
    "        \n",
    "        # 2. ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "        print(\"\\n=== ê²°ì¸¡ì¹˜ í™•ì¸ ===\")\n",
    "        missing_data = df[self.feature_columns].isnull().sum()\n",
    "        print(missing_data)\n",
    "        \n",
    "        # 3. ë¬´í•œê°’ í™•ì¸\n",
    "        print(\"\\n=== ë¬´í•œê°’ í™•ì¸ ===\")\n",
    "        for col in self.feature_columns:\n",
    "            inf_count = np.isinf(df[col]).sum()\n",
    "            print(f\"{col}: {inf_count}ê°œì˜ ë¬´í•œê°’\")\n",
    "        \n",
    "        # 4. ì‹œê°í™”\n",
    "        plt.style.use('default')\n",
    "        \n",
    "        # 4-1. íˆìŠ¤í† ê·¸ë¨\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        fig.suptitle('ë©”ì‰¬ íŠ¹ì„± ë¶„í¬ íˆìŠ¤í† ê·¸ë¨', fontsize=16)\n",
    "        \n",
    "        for i, col in enumerate(self.feature_columns):\n",
    "            row, col_idx = divmod(i, 3)\n",
    "            \n",
    "            # ì´ìƒì¹˜ ì œê±°ë¥¼ ìœ„í•œ percentile ê¸°ë°˜ í•„í„°ë§\n",
    "            q1 = df[col].quantile(0.01)\n",
    "            q99 = df[col].quantile(0.99)\n",
    "            filtered_data = df[col][(df[col] >= q1) & (df[col] <= q99)]\n",
    "            \n",
    "            axes[row, col_idx].hist(filtered_data, bins=50, alpha=0.7, edgecolor='black')\n",
    "            axes[row, col_idx].set_title(f'{col}')\n",
    "            axes[row, col_idx].set_xlabel('ê°’')\n",
    "            axes[row, col_idx].set_ylabel('ë¹ˆë„')\n",
    "            axes[row, col_idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ë¹ˆ subplot ì œê±°\n",
    "        fig.delaxes(axes[1, 2])\n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            plt.savefig('mesh_feature_histograms.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # 4-2. ë°•ìŠ¤í”Œë¡¯ (ì´ìƒì¹˜ íƒì§€)\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 6))\n",
    "        fig.suptitle('ë©”ì‰¬ íŠ¹ì„± ë°•ìŠ¤í”Œë¡¯ (ì´ìƒì¹˜ íƒì§€)', fontsize=16)\n",
    "        \n",
    "        for i, col in enumerate(self.feature_columns):\n",
    "            # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì ìš© (ì–‘ìˆ˜ ê°’ë§Œ)\n",
    "            positive_data = df[col][df[col] > 0]\n",
    "            if len(positive_data) > 0:\n",
    "                axes[i].boxplot(positive_data, vert=True)\n",
    "                axes[i].set_yscale('log')\n",
    "            else:\n",
    "                axes[i].boxplot(df[col], vert=True)\n",
    "            \n",
    "            axes[i].set_title(f'{col}')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            plt.savefig('mesh_feature_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # 4-3. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        correlation_matrix = df[self.feature_columns].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, fmt='.3f')\n",
    "        plt.title('ë©”ì‰¬ íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„')\n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            plt.savefig('mesh_feature_correlation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # 4-4. ì‚°ì ë„ (ì£¼ìš” íŠ¹ì„± ê°„ ê´€ê³„)\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        fig.suptitle('ì£¼ìš” íŠ¹ì„± ê°„ ê´€ê³„', fontsize=16)\n",
    "        \n",
    "        # area vs perimeter\n",
    "        axes[0,0].scatter(df['area'], df['perimeter'], alpha=0.5, s=1)\n",
    "        axes[0,0].set_xlabel('Area')\n",
    "        axes[0,0].set_ylabel('Perimeter')\n",
    "        axes[0,0].set_title('Area vs Perimeter')\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # aspect_ratio vs angle\n",
    "        axes[0,1].scatter(df['aspect_ratio'], df['angle'], alpha=0.5, s=1)\n",
    "        axes[0,1].set_xlabel('Aspect Ratio')\n",
    "        axes[0,1].set_ylabel('Angle')\n",
    "        axes[0,1].set_title('Aspect Ratio vs Angle')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # area vs centroid_distance\n",
    "        axes[1,0].scatter(df['area'], df['centroid_distance'], alpha=0.5, s=1)\n",
    "        axes[1,0].set_xlabel('Area')\n",
    "        axes[1,0].set_ylabel('Centroid Distance')\n",
    "        axes[1,0].set_title('Area vs Centroid Distance')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # perimeter vs aspect_ratio\n",
    "        axes[1,1].scatter(df['perimeter'], df['aspect_ratio'], alpha=0.5, s=1)\n",
    "        axes[1,1].set_xlabel('Perimeter')\n",
    "        axes[1,1].set_ylabel('Aspect Ratio')\n",
    "        axes[1,1].set_title('Perimeter vs Aspect Ratio')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            plt.savefig('mesh_feature_scatter.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def detect_anomalies(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"ì´ìƒì¹˜ íƒì§€\"\"\"\n",
    "        print(\"\\nğŸ” ì´ìƒì¹˜ íƒì§€ ì¤‘...\")\n",
    "        \n",
    "        anomalies = []\n",
    "        \n",
    "        for col in self.feature_columns:\n",
    "            # IQR ë°©ë²•\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            \n",
    "            print(f\"{col}: {len(outliers)}ê°œì˜ ì´ìƒì¹˜ ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "            \n",
    "            # ì´ìƒì¹˜ ì •ë³´ ì €ì¥\n",
    "            for idx, row in outliers.iterrows():\n",
    "                anomalies.append({\n",
    "                    'file_path': row['file_path'],\n",
    "                    'face_idx': row['face_idx'],\n",
    "                    'feature': col,\n",
    "                    'value': row[col],\n",
    "                    'lower_bound': lower_bound,\n",
    "                    'upper_bound': upper_bound\n",
    "                })\n",
    "        \n",
    "        anomaly_df = pd.DataFrame(anomalies)\n",
    "        return anomaly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ecfe2d5-10fb-4078-a808-b3ec26679f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ì‹¤í–‰ í•¨ìˆ˜\n",
    "# =============================================================================\n",
    "\n",
    "def setup_environment(source_dir: str, training_dir: str, \n",
    "                             max_vertices: int = 2048, max_faces: int = 4096):\n",
    "    \"\"\"í•™ìŠµ í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„\"\"\"\n",
    "    print(\"ğŸš€ í•™ìŠµ í™˜ê²½ ì„¤ì • ì‹œì‘\")\n",
    "    \n",
    "    # 1. ë°ì´í„° ë§¤ë‹ˆì € ìƒì„±\n",
    "    data_manager = MeshDataManager(\n",
    "        source_dir=source_dir,\n",
    "        training_dir=training_dir,\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    )\n",
    "    \n",
    "    # 2. í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "    if not data_manager.prepare_training_data():\n",
    "        return None\n",
    "    \n",
    "    # 3. íŠ¹ì„± ë¶„ì„\n",
    "    analyzer = MeshFeatureAnalyzer(data_manager)\n",
    "    feature_results = analyzer.analyze_features(num_samples=7)\n",
    "    \n",
    "    # 4. ì‹œê°í™”\n",
    "    analyzer.visualize_features(feature_results)\n",
    "    \n",
    "    # 5. ìš”ì•½ ì •ë³´ ì €ì¥\n",
    "    summary = {\n",
    "        'setup_completed': True,\n",
    "        'source_dir': source_dir,\n",
    "        'training_dir': training_dir,\n",
    "        'max_vertices': max_vertices,\n",
    "        'max_faces': max_faces,\n",
    "        'datasets_analyzed': list(feature_results.keys()),\n",
    "        'total_features_analyzed': sum(len(df) for df in feature_results.values())\n",
    "    }\n",
    "    \n",
    "    with open(data_manager.training_dir / \"setup_summary.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(\"âœ… í•™ìŠµ í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬: {training_dir}\")\n",
    "    print(f\"ğŸ“Š ë¶„ì„ ê²°ê³¼: {data_manager.analysis_dir}\")\n",
    "    \n",
    "    return data_manager\n",
    "\n",
    "def create_training_datasets(training_dir: str, batch_size: int = 8, \n",
    "                           max_vertices: int = 5000, max_faces: int = 10000):\n",
    "    \"\"\"í•™ìŠµìš© TensorFlow ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    print(\"ğŸ”„ TensorFlow ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    training_path = Path(training_dir)\n",
    "    cache_dir = training_path / \"cache\"\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ìƒì„±\n",
    "    train_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"train\"),\n",
    "        cache_dir=str(cache_dir / \"train\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"val\"),\n",
    "        cache_dir=str(cache_dir / \"val\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    test_dataset = MeshDatasetTF(\n",
    "        data_dir=str(training_path / \"test\"),\n",
    "        cache_dir=str(cache_dir / \"test\"),\n",
    "        max_vertices=max_vertices,\n",
    "        max_faces=max_faces\n",
    "    ).create_dataset(batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!\")\n",
    "    print(f\"  - ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "    print(f\"  - ìµœëŒ€ ì •ì  ìˆ˜: {max_vertices}\")\n",
    "    print(f\"  - ìµœëŒ€ ë©´ ìˆ˜: {max_faces}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac441b1-a88d-41d5-9011-c1b85f0e87b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ë©”ì‰¬ ë°ì´í„° í•™ìŠµ í™˜ê²½ ì„¤ì •\n",
      "============================================================\n",
      "ğŸ”§ í•˜ë“œì›¨ì–´ ì„¤ì •: ì •ì =2048, ë©´=4096, ë°°ì¹˜=8\n",
      "ğŸš€ í•™ìŠµ í™˜ê²½ ì„¤ì • ì‹œì‘\n",
      "ğŸ“ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...\n",
      "ì´ 75ê°œì˜ ë©”ì‰¬ íŒŒì¼ ë°œê²¬\n",
      "train ì„¸íŠ¸: 52ê°œ íŒŒì¼ ë³µì‚¬ ì¤‘...\n",
      "validation ì„¸íŠ¸: 15ê°œ íŒŒì¼ ë³µì‚¬ ì¤‘...\n",
      "test ì„¸íŠ¸: 8ê°œ íŒŒì¼ ë³µì‚¬ ì¤‘...\n",
      "âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ!\n",
      "  - í•™ìŠµ: 52ê°œ\n",
      "  - ê²€ì¦: 15ê°œ\n",
      "  - í…ŒìŠ¤íŠ¸: 8ê°œ\n",
      "ğŸ“Š 7ê°œ ìƒ˜í”Œì˜ íŠ¹ì„±ì„ ë¶„ì„ì¤‘...\n",
      "ì§„í–‰ë¥ : 1/7\n",
      "âš ï¸ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨ C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\ê²°ì ˆ ëª¨ë¸ë§ npy\\A0083.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "âš ï¸ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨ C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\ê²°ì ˆ ëª¨ë¸ë§ npy\\A0087.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "âš ï¸ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨ C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\ê²°ì ˆ ëª¨ë¸ë§ npy\\A0103.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "âš ï¸ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨ C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\ê²°ì ˆ ëª¨ë¸ë§ npy\\A0106.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "âš ï¸ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨ C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\ê²°ì ˆ ëª¨ë¸ë§ npy\\A0117.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "âš ï¸ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨ C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\ê²°ì ˆ ëª¨ë¸ë§ npy\\A0126.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "âš ï¸ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨ C:\\Users\\konyang\\Desktop\\project2025\\model\\data\\npy\\ê²°ì ˆ ëª¨ë¸ë§ npy\\A0165.npy: 'MeshDataManager' object has no attribute 'loader'\n",
      "âœ… ì´ 0ê°œì˜ face íŠ¹ì„±ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“ˆ íŠ¹ì„± ë°ì´í„° ì‹œê°í™” ì¤‘...\n",
      "\n",
      "=== ê¸°ë³¸ í†µê³„ ì •ë³´ ===\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['area', 'perimeter', 'aspect_ratio', 'angle', 'centroid_distance'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ”§ í•˜ë“œì›¨ì–´ ì„¤ì •: ì •ì =\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_VERTICES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ë©´=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_FACES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ë°°ì¹˜=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 1. í•™ìŠµ í™˜ê²½ ì„¤ì •\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m data_manager \u001b[38;5;241m=\u001b[39m \u001b[43msetup_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSOURCE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAINING_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_vertices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_VERTICES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_faces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_FACES\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ í•™ìŠµ í™˜ê²½ ì„¤ì • ì‹¤íŒ¨\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m, in \u001b[0;36msetup_environment\u001b[1;34m(source_dir, training_dir, max_vertices, max_faces)\u001b[0m\n\u001b[0;32m     24\u001b[0m feature_results \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39manalyze_features(num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 4. ì‹œê°í™”\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 5. ìš”ì•½ ì •ë³´ ì €ì¥\u001b[39;00m\n\u001b[0;32m     30\u001b[0m summary \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetup_completed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_dir\u001b[39m\u001b[38;5;124m'\u001b[39m: source_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_features_analyzed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m feature_results\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     38\u001b[0m }\n",
      "Cell \u001b[1;32mIn[6], line 59\u001b[0m, in \u001b[0;36mMeshFeatureAnalyzer.visualize_features\u001b[1;34m(self, df, save_plots)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 1. ê¸°ë³¸ í†µê³„ ì •ë³´\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== ê¸°ë³¸ í†µê³„ ì •ë³´ ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe())\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# 2. ê²°ì¸¡ì¹˜ í™•ì¸\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== ê²°ì¸¡ì¹˜ í™•ì¸ ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\project2025\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\project2025\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\project2025\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['area', 'perimeter', 'aspect_ratio', 'angle', 'centroid_distance'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ì„¤ì •\n",
    "    BASE_DIR = \"C:/Users/konyang/Desktop/project2025/model/data/npy\"\n",
    "    SOURCE_DIR = os.path.join(BASE_DIR, \"ê²°ì ˆ ëª¨ë¸ë§ npy\")  # ì›ë³¸ ë©”ì‰¬ íŒŒì¼ ë””ë ‰í† ë¦¬\n",
    "    TRAINING_DIR = \"C:/Users/konyang/Desktop/MeshCNN_TF/data/train\"      # í•™ìŠµ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "    \n",
    "    # í•™ìŠµ ì„¤ì • - í™˜ê²½ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”\n",
    "    '''\n",
    "    ê³ ì„±ëŠ¥ GPU (24GB+): MAX_VERTICES=5000, MAX_FACES=10000, BATCH_SIZE=32\n",
    "    ì¤‘ê°„ GPU (8-16GB): MAX_VERTICES=3000, MAX_FACES=6000, BATCH_SIZE=16  \n",
    "    ì €ì‚¬ì–‘ GPU (4-8GB): MAX_VERTICES=1500, MAX_FACES=3000, BATCH_SIZE=8\n",
    "    '''\n",
    "    MAX_VERTICES = 2048\n",
    "    MAX_FACES = 4096\n",
    "    BATCH_SIZE = 8\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ë©”ì‰¬ ë°ì´í„° í•™ìŠµ í™˜ê²½ ì„¤ì •\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ”§ í•˜ë“œì›¨ì–´ ì„¤ì •: ì •ì ={MAX_VERTICES}, ë©´={MAX_FACES}, ë°°ì¹˜={BATCH_SIZE}\")\n",
    "    \n",
    "    # 1. í•™ìŠµ í™˜ê²½ ì„¤ì •\n",
    "    data_manager = setup_environment(\n",
    "        source_dir=SOURCE_DIR,\n",
    "        training_dir=TRAINING_DIR,\n",
    "        max_vertices=MAX_VERTICES,\n",
    "        max_faces=MAX_FACES\n",
    "    )\n",
    "    \n",
    "    if data_manager is None:\n",
    "        print(\"âŒ í•™ìŠµ í™˜ê²½ ì„¤ì • ì‹¤íŒ¨\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 2. í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±\n",
    "    train_ds, val_ds, test_ds = create_datasets(\n",
    "        training_dir=TRAINING_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_vertices=MAX_VERTICES,\n",
    "        max_faces=MAX_FACES\n",
    "    )\n",
    "    \n",
    "    # 3. ë°ì´í„°ì…‹ í™•ì¸\n",
    "    print(\"\\nğŸ” ë°ì´í„°ì…‹ í™•ì¸:\")\n",
    "    for batch in train_ds.take(1):\n",
    "        print(f\"Batch shape:\")\n",
    "        for key, value in batch.items():\n",
    "            print(f\"  {key}: {value.shape}\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ ëª¨ë“  ì¤€ë¹„ ì™„ë£Œ! ì´ì œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ğŸ“‚ í•™ìŠµ ë°ì´í„°: {TRAINING_DIR}\")\n",
    "    print(f\"ğŸ“Š ë¶„ì„ ê²°ê³¼: {TRAINING_DIR}/analysis/\")\n",
    "    print(f\"ğŸ’¡ ì„¤ì •ì„ ë³€ê²½í•˜ë ¤ë©´ ìœ„ì˜ MAX_VERTICES, MAX_FACES, BATCH_SIZE ê°’ì„ ì¡°ì •í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85826a3-18d9-44db-bc14-647124037823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2025",
   "language": "python",
   "name": "project2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
